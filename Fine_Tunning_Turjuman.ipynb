{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedAzzam4/Turjuman-AI/blob/main/Fine_Tunning_Turjuman.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abrW95rOROb3"
      },
      "source": [
        "# Setup Environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZRg9eNJW8l7h",
        "outputId": "406aad24-a981-4192-c57a-a47ed7324975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.51.3\n",
            "Uninstalling transformers-4.51.3:\n",
            "  Successfully uninstalled transformers-4.51.3\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# removing any damaged version of transformers\n",
        "!pip uninstall -y transformers\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/transformers*\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/~ransformers*\n",
        "\n",
        "# instaling a comabtiable version of transformers\n",
        "!pip install -qU transformers==4.41.2\n",
        "\n",
        "# installing numpy combatiable with numpa and tensorflow\n",
        "!pip install numpy==1.26.4 --quiet\n",
        "\n",
        "# Intsallaing the rest of libraries\n",
        "!pip install -qU pydantic\n",
        "!pip install -qU sentencepiece accelerate bitsandbytes\n",
        "!pip install -qU json-repair==0.29.1\n",
        "!pip install -qU langchain langchain-community\n",
        "\n",
        "!pip install -q langchain_google_genai\n",
        "#restart runtime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Clone and install LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!pip install -e LLaMA-Factory"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3teVBdCV__lc",
        "outputId": "8631288f-9fd9-454b-a0ed-14c831fda4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 357, done.\u001b[K\n",
            "remote: Counting objects: 100% (357/357), done.\u001b[K\n",
            "remote: Compressing objects: 100% (287/287), done.\u001b[K\n",
            "remote: Total 357 (delta 94), reused 166 (delta 55), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (357/357), 9.71 MiB | 26.67 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting datasets<=3.5.0,>=2.16.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.6.0)\n",
            "Collecting peft<=0.15.1,>=0.14.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading peft-0.15.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.19.1)\n",
            "Collecting gradio<=5.25.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio-5.25.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.15.2)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Collecting tiktoken (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
            "Collecting uvicorn (from llamafactory==0.9.3.dev0)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting fastapi (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
            "  Downloading sse_starlette-2.3.3-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Collecting fire (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting omegaconf (from llamafactory==0.9.3.dev0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
            "Collecting pydantic<=2.10.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Collecting av (from llamafactory==0.9.3.dev0)\n",
            "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Collecting xxhash (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Collecting ffmpy (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.2.1)\n",
            "Collecting pydub (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ruff-0.11.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Collecting tokenizers<=0.21.1,>=0.19.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->llamafactory==0.9.3.dev0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.20.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.9)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.19.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.25.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.25.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.1-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.0/411.0 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.3.3-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.7-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire, antlr4-python3-runtime\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26674 sha256=8c4b65cd5597801a30691e7b9d86c580e081147664b1cf01493f8699b304d6f2\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-giyxxu6u/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=2becd8ee49324fe2b4f6fbd39d5c976d40a3174a257dc8afc6cb9cd0a4fc4904\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=6bb7a6c5e23f518e2b9c5c2a50bcdcbad76157928d4689e03a87ac2d4274031d\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built llamafactory fire antlr4-python3-runtime\n",
            "Installing collected packages: pydub, antlr4-python3-runtime, xxhash, uvicorn, tomlkit, shtab, semantic-version, ruff, python-multipart, pydantic-core, omegaconf, groovy, fsspec, fire, ffmpy, dill, av, aiofiles, tiktoken, starlette, pydantic, multiprocess, tyro, tokenizers, sse-starlette, safehttpx, gradio-client, fastapi, transformers, gradio, datasets, trl, peft, llamafactory\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.1\n",
            "    Uninstalling pydantic_core-2.33.1:\n",
            "      Successfully uninstalled pydantic_core-2.33.1\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.3\n",
            "    Uninstalling pydantic-2.11.3:\n",
            "      Successfully uninstalled pydantic-2.11.3\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.15.2\n",
            "    Uninstalling peft-0.15.2:\n",
            "      Successfully uninstalled peft-0.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-24.1.0 antlr4-python3-runtime-4.9.3 av-14.3.0 datasets-3.5.0 dill-0.3.8 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.12.0 gradio-5.25.0 gradio-client-1.8.0 groovy-0.1.2 llamafactory-0.9.3.dev0 multiprocess-0.70.16 omegaconf-2.3.0 peft-0.15.1 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.7 safehttpx-0.1.6 semantic-version-2.10.0 shtab-1.7.2 sse-starlette-2.3.3 starlette-0.46.2 tiktoken-0.9.0 tokenizers-0.21.1 tomlkit-0.13.2 transformers-4.51.3 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.2 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "b0a01d90a9c74e799c077046bff264dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "id": "XiS_URle5nQd",
        "outputId": "25926333-adc1-4995-a248-bd35b8502336"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0aa6cc4f97ce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "from pydantic import BaseModel, Field, ValidationError\n",
        "from typing import List, Optional, Literal\n",
        "\n",
        "import json\n",
        "import json_repair\n",
        "\n",
        "import os\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import json\n",
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "from os.path import join\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGxqV5PYRXPd"
      },
      "source": [
        "## Setting up Qwen Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzxm0dY7RjK-"
      },
      "source": [
        "### Downloading model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "b690dc04d98141208005679849a44bbc",
            "984f2eb730414999980ad2c194a48905",
            "cd52f0ed0e7f425b8f496271b0291344",
            "1319d3163a944c6f931a0c1012edaf20",
            "7c7c1585c5d0402d85716fef9079e0b9",
            "8c634303bc314755837bfd524cbb01f4",
            "8c67280c5b254fd2ab07a951403df76f",
            "795db8d889444f41bc9437dd527bd7d2",
            "37d61b0cc7774621a23b0268e9bd5ac2",
            "6ed2f1a5fc11473fac4bee3c3399232c",
            "331543e3cc594f1a842dc16089f2d802",
            "5cbe6af3f46f4055a942805f1f42a8a6",
            "d2f7c9137e95401aae2d9b460ec0ef90",
            "072510185bcb4fc4853930a1573cfba1",
            "cddb5bda3898441485c814e43c8dee29",
            "2ca5d9b540344d4caf22ec494104a55c",
            "b9c861ea25a84dc496f1a7da781ad77a",
            "0b7138003452480a93ab3610941c3056",
            "663754bbbfcf4e1983acf014b5828434",
            "0ea96efb33554be9baf3bc0aaff82e5d",
            "7c6341cec8374444b7d1518f826d23e1",
            "51ffd949e7ad46148780456192906f65"
          ]
        },
        "id": "XvWLwr478xL9",
        "outputId": "6d540e95-7607-4d98-8fae-1ad7747e0ac4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b690dc04d98141208005679849a44bbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/3.09G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5cbe6af3f46f4055a942805f1f42a8a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_id = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
        "\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = None\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gtbcAPmRz_m"
      },
      "source": [
        "### Settings of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybJgn3X68yu_",
        "outputId": "aa35a2b6-98fc-4cfe-cc68-7b3f772b99fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-33b910385fc3>:9: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"text-generation\",\n",
        "                model= model,\n",
        "                tokenizer=tokenizer,\n",
        "                max_length=256,\n",
        "                temperature=0.6,\n",
        "                top_p=0.95,\n",
        "                repetition_penalty=1.2)\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUT3MZSaxnox"
      },
      "source": [
        "## Gemini model setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhyfl-T3rQB7"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "gemini_model = ChatGoogleGenerativeAI(\n",
        "    model=\"models/gemini-1.5-flash\",\n",
        "    temperature=0.5,\n",
        "    google_api_key=\"AIzaSyCV5NIPdognzAB1S2-Q9LtS2BiKjnY5k28\"\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1vllj7oR6jo"
      },
      "source": [
        "## Taking into Action To Translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTl4WZiqzgYd"
      },
      "outputs": [],
      "source": [
        "word =\"result\"\n",
        "\n",
        "context = \"During his first presidency, Trump imposed a travel ban on seven Muslim-majority countries,\\\n",
        " expanded the Mexico–United States border wall,\\\n",
        " and enforced a family separation policy on the border. \\\n",
        " He rolled back environmental and business regulations, \\\n",
        " signed the Tax Cuts and Jobs Act, and appointed three Supreme Court justices. \\\n",
        " In foreign policy, Trump withdrew the U.S. from agreements on climate, trade, \\\n",
        " and Iran's nuclear program, and initiated a trade war with China. \\\n",
        " In response to the COVID-19 pandemic from 2020, he downplayed its severity, \\\n",
        " contradicted health officials, and signed the CARES Act. After losing the 2020 presidential election to Joe Biden, \\\n",
        " Trump attempted to overturn the result, culminating in the January 6 Capitol attack in 2021. \\\n",
        " Trump was impeached in 2019 for abuse of power and obstruction of Congress, \\\n",
        " and in 2021 for incitement of insurrection; the Senate acquitted him both times. \\\n",
        " After his first term, scholars and historians ranked him as one of the worst presidents in American history.\"\n",
        "\n",
        "# word = input(\"Enter the word you want to translate: \")\n",
        "# context = input(\"Enter the context of the word\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYOV5SYRTStS",
        "outputId": "1e11b4ff-13f8-4bd8-e649-73469d73b684"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-b96ede04c6ef>:63: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  json.dumps(Translation.schema(), ensure_ascii=False),\n"
          ]
        }
      ],
      "source": [
        "class Translation(BaseModel):\n",
        "    translated_word: str = Field(\n",
        "        ...,\n",
        "        min_length=2,\n",
        "        max_length=255,\n",
        "        description=\"The translated word in the target language, based on the provided context.\"\n",
        "    )\n",
        "    target_synonyms: List[str] = Field(\n",
        "        ...,\n",
        "        min_items=1,\n",
        "        max_items=5,\n",
        "        description=\"Different synonymous words in Arabic, relevant to the context. No duplicates.\"\n",
        "    )\n",
        "    source_synonyms: List[str] = Field(\n",
        "        ...,\n",
        "        min_items=1,\n",
        "        max_items=5,\n",
        "        description=\"Synonyms of the word in English. No duplicates.\"\n",
        "    )\n",
        "    definition: str = Field(\n",
        "        ...,\n",
        "        min_length=5,\n",
        "        max_length=255,\n",
        "        description=\"Definition of the original word in English.\"\n",
        "    )\n",
        "    example_usage: str = Field(\n",
        "        ...,\n",
        "        min_length=5,\n",
        "        max_length=255,\n",
        "        description=\"An example sentence or phrase using the word to demonstrate its usage in context in English.\"\n",
        "    )\n",
        "\n",
        "def translation(word, context):\n",
        "  translation_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"You are a professional translator from English to Arabic.\",\n",
        "                \"You will be provided with an English word that needs translation.\",\n",
        "                \"You will also receive a context paragraph to help with accurate translation.\",\n",
        "                \"Translate the word based on the context.\",\n",
        "                \"Provide:\",\n",
        "                \"- The translated word in Arabic.\",\n",
        "                \"- A list of up to 5 different synonyms in Arabic (target language).\",\n",
        "                \"- A list of up to 5 different synonyms in English (source language).\",\n",
        "                \"- The English definition of the original word.\",\n",
        "                \"- An example sentence that uses the word naturally in context in English.\",\n",
        "                \"Your output must strictly follow the JSON format as per the given schema:\",\n",
        "                \"{ translated_word, target_synonyms, source_synonyms, definition, example_usage }\",\n",
        "                \"Do not add explanations, markdown, or extra text — only return valid JSON.\",\n",
        "            ])\n",
        "        },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"\\n\".join([\n",
        "                                    \"## The Context:\",\n",
        "                                    context.strip(),\n",
        "\n",
        "                                    \"## The Word:\",\n",
        "                                    word.strip(),\n",
        "\n",
        "                                    \"## The pydatic Scheme:\",\n",
        "                                    json.dumps(Translation.schema(), ensure_ascii=False),\n",
        "\n",
        "                                    \"## Translated Word:\",\n",
        "                                    \"```json\"\n",
        "              ])\n",
        "          }\n",
        "      ]\n",
        "  return translation_messages\n",
        "\n",
        "translation_messages = translation(word, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFoDByTsx1U3"
      },
      "source": [
        "### Qwen Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0bjz2Z8gu-M",
        "outputId": "82c18be8-6762-4a25-a3f2-81f0a1db967d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "```json\n",
            "{\n",
            "  \"translated_word\": \"نتيجة\",\n",
            "  \"target_synonyms\": [\"نتيجة\", \"نتيجة\", \"نتيجة\", \"نتيجة\", \"نتيجة\"],\n",
            "  \"source_synonyms\": [\"نتيجة\", \"نتيجة\", \"نتيجة\", \"نتيجة\", \"نتيجة\"],\n",
            "  \"definition\": \"النتيجة أو النتيجة النهائية أو النتيجة الناجمة عن شيء ما.\",\n",
            "  \"example_usage\": \"النتيجة النهائية لهذا التحدي كانت إجراءات أكثر تحديداً وفعالية.\"\n",
            "}\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "#print(text)\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) # pt = return tensors in pytorch\n",
        "# print(model_inputs)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=512,\n",
        "    do_sample=None, temperature=None, top_p=None, repetition_penalty=None\n",
        ") # Here the output returned will be input ids + output ids\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[  len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "] # output ids only\n",
        "\n",
        "qwen_response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] # skip speacial tokens to hide (im start|)\n",
        "print(qwen_response)\n",
        "print('='*25)\n",
        "#############################################\n",
        "\n",
        "# Parsing the output\n",
        "paresed_response_qwen = parse_json(qwen_response)\n",
        "print( paresed_response_qwen )\n",
        "print( paresed_response_qwen['translated_word'] )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1pWUNdIrNmM"
      },
      "source": [
        "### Gemini Model Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyxvx_Ooydmb"
      },
      "outputs": [],
      "source": [
        "gemini_response = gemini_model.invoke(translation_messages)\n",
        "print(gemini_response.content)\n",
        "print('#'*25)\n",
        "\n",
        "# Parsing the output\n",
        "parsed_response_gemini = parse_json(gemini_response.content)\n",
        "print(parsed_response_gemini['translated_word'])\n",
        "print(parsed_response_gemini['translated_word'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-9hLYmcUOvG"
      },
      "source": [
        "## Gemini and GPT Model by Requesty API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsSG25LzA0d3",
        "outputId": "f3eeae0a-0d9d-47cc-8aa1-6cbe99937ebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6P4_7CBP2nh",
        "outputId": "8295129d-42bf-48a8-b8bf-d211afc1d6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I help you today?\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import openai\n",
        "#from dotenv import load_dotenv\n",
        "\n",
        "# Load API key from environment variables\n",
        "#load_dotenv()\n",
        "ROUTER_API_KEY = \"sk-b9QHhuZ2TLGW0mMWadkX0PkztZ8R0Hdo15GYzMeyBSdQViCTRYwh0LN2ZKeNvhMBdyvtDNbWl3D6e9rZVkiLw9gAMy+wkzBZ6fuqGv4YJgo=\"\n",
        "gpt_model_name = 'openai/gpt-4o-mini-2024-07-18'\n",
        "gemini_model_name = 'google/gemini-2.5-flash-preview-04-17'\n",
        "\n",
        "try:\n",
        "    # Initialize OpenAI client\n",
        "    client = openai.OpenAI(\n",
        "        api_key=ROUTER_API_KEY,\n",
        "        base_url=\"https://router.requesty.ai/v1\",\n",
        "        default_headers={\"Authorization\": f\"Bearer {ROUTER_API_KEY}\"}\n",
        "    )\n",
        "\n",
        "    # Example request\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"google/gemini-2.5-flash-preview-04-17\",\n",
        "        messages= [\n",
        "    {\"role\": \"user\", \"content\": \"HI\"} ]\n",
        "\n",
        "    )\n",
        "\n",
        "    # Check if the response is successful\n",
        "    if not response.choices:\n",
        "        raise Exception(\"No response choices found.\")\n",
        "\n",
        "    # Print the result\n",
        "    print(response.choices[0].message.content)\n",
        "\n",
        "except openai.OpenAIError as e:\n",
        "    print(f\"OpenAI API error: {e}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbUZ1wJrUsU0"
      },
      "outputs": [],
      "source": [
        "prompt_tokens = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJhyHnsrQeKe"
      },
      "outputs": [],
      "source": [
        "response = client.chat.completions.create(\n",
        "        model=\"google/gemini-2.5-flash-preview-04-17\",\n",
        "        messages= translation_messages\n",
        "    )\n",
        "\n",
        "if response.choices[0].finish_reason != \"stop\":\n",
        "    prompt_tokens += response.usage.prompt_tokens\n",
        "\n",
        "\n",
        "llm_response = response.choices[0].message.content\n",
        "print(parse_json(llm_response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QhEwcFUVcEl",
        "outputId": "5db0452a-e1b6-472d-da4b-c83c4fe55824"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'translated_word': 'نتيجة',\n",
              " 'target_synonyms': ['حصيلة', 'مُحصّلة', 'مآل', 'عاقبة', 'خلاصة'],\n",
              " 'source_synonyms': ['outcome',\n",
              "  'consequence',\n",
              "  'effect',\n",
              "  'upshot',\n",
              "  'conclusion'],\n",
              " 'definition': 'A consequence, effect, or outcome of something. The final score, outcome, or conclusion of a competition, game, or election.',\n",
              " 'example_usage': 'After losing the 2020 presidential election to Joe Biden, Trump attempted to overturn the result.'}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(llm_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-Ny_bygzBCX"
      },
      "source": [
        "# Strart Fine Tunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zgxLw006RLi"
      },
      "source": [
        "## Knowledge distlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mpK4-NaDvbP"
      },
      "source": [
        "### Setup LLM Teacher\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQg7VtZaD2Z7"
      },
      "outputs": [],
      "source": [
        "ROUTER_API_KEY = \"sk-b9QHhuZ2TLGW0mMWadkX0PkztZ8R0Hdo15GYzMeyBSdQViCTRYwh0LN2ZKeNvhMBdyvtDNbWl3D6e9rZVkiLw9gAMy+wkzBZ6fuqGv4YJgo=\"\n",
        "gpt_model_name = 'openai/gpt-4o-mini-2024-07-18'\n",
        "gemini_model_name = 'google/gemini-2.5-flash-preview-04-17'\n",
        "\n",
        "client = openai.OpenAI(\n",
        "        api_key= ROUTER_API_KEY,\n",
        "        base_url=\"https://router.requesty.ai/v1\",\n",
        "        default_headers={\"Authorization\": f\"Bearer {ROUTER_API_KEY}\"}\n",
        "    )\n",
        "\n",
        "    # Example request\n",
        "response = client.chat.completions.create(\n",
        "    model= gpt_model_name,\n",
        "    messages= [\n",
        "{\"role\": \"user\", \"content\": \"HI\"} ]\n",
        "\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojpMBQKtEF3h",
        "outputId": "13458e97-6957-405f-b8a9-5d77f826f0bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! How can I assist you today?\n"
          ]
        }
      ],
      "source": [
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJTFm4S7E1VH"
      },
      "source": [
        "### Setup the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0JC1LCOFLyF"
      },
      "source": [
        "#### Get Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VdEzMHSQzDQB",
        "outputId": "ece999e5-f356-4f3e-f934-e80aec093b9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw data: 18563\n"
          ]
        }
      ],
      "source": [
        "raw_data_path = \"/content/drive/MyDrive/finetunning/predata.jsonl\"\n",
        "\n",
        "raw_data = []\n",
        "for line in open(raw_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    raw_data.append(\n",
        "        json.loads(line.strip())\n",
        "    )\n",
        "\n",
        "random.Random(101).shuffle(raw_data)\n",
        "\n",
        "print(f\"Raw data: {len(raw_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "zX60zc5lJRG1",
        "outputId": "6c9f0167-09a0-42bf-b19b-fcbb7334624c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'academia'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_data[0]['word']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWp54_q6FQpf"
      },
      "source": [
        "#### Resetup the Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya7pVpYFFTnw"
      },
      "outputs": [],
      "source": [
        "class Translation(BaseModel):\n",
        "    translated_word: str = Field(\n",
        "        ...,\n",
        "        min_length=2,\n",
        "        max_length=255,\n",
        "        description=\"The translated word in the target language, based on the provided context.\"\n",
        "    )\n",
        "    target_synonyms: List[str] = Field(\n",
        "        ...,\n",
        "        min_items=1,\n",
        "        max_items=5,\n",
        "        description=\"Different synonymous words in Arabic, relevant to the context. No duplicates.\"\n",
        "    )\n",
        "    source_synonyms: List[str] = Field(\n",
        "        ...,\n",
        "        min_items=1,\n",
        "        max_items=5,\n",
        "        description=\"Synonyms of the word in English. No duplicates.\"\n",
        "    )\n",
        "    definition: str = Field(\n",
        "        ...,\n",
        "        min_length=5,\n",
        "        max_length=255,\n",
        "        description=\"Definition of the original word in English.\"\n",
        "    )\n",
        "    example_usage: str = Field(\n",
        "        ...,\n",
        "        min_length=5,\n",
        "        max_length=255,\n",
        "        description=\"An example sentence or phrase using the word to demonstrate its usage in context in English.\"\n",
        "    )\n",
        "\n",
        "def translation(word, context):\n",
        "  translation_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"You are a professional translator from English to Arabic.\",\n",
        "                \"You will be provided with an English word that needs translation.\",\n",
        "                \"You will also receive a context paragraph to help with accurate translation.\",\n",
        "                \"Translate the word based on the context.\",\n",
        "                \"Provide:\",\n",
        "                \"- The translated word in Arabic.\",\n",
        "                \"- A list of up to 5 different synonyms in Arabic (target language).\",\n",
        "                \"- A list of up to 5 different synonyms in English (source language).\",\n",
        "                \"- The English definition of the original word.\",\n",
        "                \"- An example sentence that uses the word naturally in context in English.\",\n",
        "                \"Your output must strictly follow the JSON format as per the given schema:\",\n",
        "                \"{ translated_word, target_synonyms, source_synonyms, definition, example_usage }\",\n",
        "                \"Do not add explanations, markdown, or extra text — only return valid JSON.\",\n",
        "            ])\n",
        "        },\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": \"\\n\".join([\n",
        "                                    \"## The Context:\",\n",
        "                                    context.strip(),\n",
        "\n",
        "                                    \"## The Word:\",\n",
        "                                    word.strip(),\n",
        "\n",
        "                                    \"## The pydatic Scheme:\",\n",
        "                                    json.dumps(Translation.schema(), ensure_ascii=False),\n",
        "\n",
        "                                    \"## Translated Word:\",\n",
        "                                    \"```json\"\n",
        "              ])\n",
        "          }\n",
        "      ]\n",
        "  return translation_messages\n",
        "\n",
        "## example usage :\n",
        "# translation_messages = translation(word, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "i1gRrtVsIp6l",
        "outputId": "0e7e7f74-5478-4f75-bc6a-d513f70fbd41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'properties': {'translated_word': {'description': 'The translated word in the target language, based on the provided context.',\n",
              "   'maxLength': 255,\n",
              "   'minLength': 2,\n",
              "   'title': 'Translated Word',\n",
              "   'type': 'string'},\n",
              "  'target_synonyms': {'description': 'Different synonymous words in Arabic, relevant to the context. No duplicates.',\n",
              "   'items': {'type': 'string'},\n",
              "   'maxItems': 5,\n",
              "   'minItems': 1,\n",
              "   'title': 'Target Synonyms',\n",
              "   'type': 'array'},\n",
              "  'source_synonyms': {'description': 'Synonyms of the word in English. No duplicates.',\n",
              "   'items': {'type': 'string'},\n",
              "   'maxItems': 5,\n",
              "   'minItems': 1,\n",
              "   'title': 'Source Synonyms',\n",
              "   'type': 'array'},\n",
              "  'definition': {'description': 'Definition of the original word in English.',\n",
              "   'maxLength': 255,\n",
              "   'minLength': 5,\n",
              "   'title': 'Definition',\n",
              "   'type': 'string'},\n",
              "  'example_usage': {'description': 'An example sentence or phrase using the word to demonstrate its usage in context in English.',\n",
              "   'maxLength': 255,\n",
              "   'minLength': 5,\n",
              "   'title': 'Example Usage',\n",
              "   'type': 'string'}},\n",
              " 'required': ['translated_word',\n",
              "  'target_synonyms',\n",
              "  'source_synonyms',\n",
              "  'definition',\n",
              "  'example_usage'],\n",
              " 'title': 'Translation',\n",
              " 'type': 'object'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Translation.model_json_schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwKnwB6sKmC6"
      },
      "outputs": [],
      "source": [
        "def change_api_key(counter):\n",
        "  api_keys = ['sk-R06la+UPRL+6ghEf0lnsQ72LDbnVKYkk2NQYaoT/4iWHfeqNxRAJdhTcxOk53zUIrshImQRicpXZV+AWdUENrN1Nag3lb6i0z7hVbrP4wwI=',\n",
        "              'sk-tjO8mNgKRTmFDBKOdoij7Xg/nJIMRVAYPnoin0vmCw1YrpuuXdFm27ZQ/GZoC1XkTXePR0ob155tAgF9TG8ZJ99A5TLGBJEMuY1qNBD0lQQ=',\n",
        "              'sk-tjO8mNgKRTmFDBKOdoij7Xg/nJIMRVAYPnoin0vmCw1YrpuuXdFm27ZQ/GZoC1XkTXePR0ob155tAgF9TG8ZJ99A5TLGBJEMuY1qNBD0lQQ='\n",
        "              ]\n",
        "  ROUTER_API_KEY = api_keys[counter]\n",
        "  gpt_model_name = 'openai/gpt-4o-mini-2024-07-18'\n",
        "  gemini_model_name = 'google/gemini-2.5-flash-preview-04-17'\n",
        "\n",
        "  client = openai.OpenAI(\n",
        "        api_key= ROUTER_API_KEY,\n",
        "        base_url=\"https://router.requesty.ai/v1\",\n",
        "        default_headers={\"Authorization\": f\"Bearer {ROUTER_API_KEY}\"}\n",
        "    )\n",
        "  print(\"api key has changed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 940,
          "referenced_widgets": [
            "aeb7b16a93044fc388a57d2b70b8d0ba",
            "068a06a71149482f9f472cbb273cabad",
            "96d706471cbc4f768df86f791aca3eff",
            "4a0b8636b7374d259cb23d943fbb40d4",
            "fa4174ff50564676be17549f5ebf34c2",
            "c3a1927bec14451798d8188b39718632",
            "65c07644750d4e94aa3faee6442cb3f4",
            "fb5bf31e024a40d1a512d40415dc272e",
            "186d0873d0144961a905eaa9568c77cc",
            "049a0b7c43fc440ab839c807e4ca1640",
            "78ef546ad13d4948b2705f67aa72fc83"
          ]
        },
        "id": "oSuE4BiM7dQ5",
        "outputId": "7e699654-8515-4a10-8f42-ab3668cb8b37"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aeb7b16a93044fc388a57d2b70b8d0ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-e56d1385a4c1>:63: PydanticDeprecatedSince20: The `schema` method is deprecated; use `model_json_schema` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
            "  json.dumps(Translation.schema(), ensure_ascii=False),\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 3: Total Cost = $0.0005 \n",
            "Iteration 6: Total Cost = $0.0010 \n",
            "Iteration 9: Total Cost = $0.0014 \n",
            "Iteration 12: Total Cost = $0.0019 \n",
            "Iteration 15: Total Cost = $0.0024 \n",
            "Iteration 18: Total Cost = $0.0028 \n",
            "Iteration 21: Total Cost = $0.0033 \n",
            "Iteration 24: Total Cost = $0.0038 \n",
            "Iteration 27: Total Cost = $0.0042 \n",
            "Iteration 30: Total Cost = $0.0047 \n",
            "Iteration 33: Total Cost = $0.0052 \n",
            "Iteration 36: Total Cost = $0.0056 \n",
            "Iteration 39: Total Cost = $0.0061 \n",
            "Iteration 42: Total Cost = $0.0065 \n",
            "Iteration 45: Total Cost = $0.0070 \n",
            "Iteration 48: Total Cost = $0.0075 \n",
            "Iteration 51: Total Cost = $0.0079 \n",
            "Iteration 54: Total Cost = $0.0084 \n",
            "Iteration 57: Total Cost = $0.0089 \n",
            "Iteration 60: Total Cost = $0.0094 \n",
            "Iteration 63: Total Cost = $0.0099 \n",
            "Iteration 66: Total Cost = $0.0103 \n",
            "Iteration 69: Total Cost = $0.0108 \n",
            "Iteration 72: Total Cost = $0.0112 \n",
            "Iteration 75: Total Cost = $0.0117 \n",
            "Iteration 78: Total Cost = $0.0122 \n",
            "Iteration 81: Total Cost = $0.0127 \n",
            "Iteration 84: Total Cost = $0.0131 \n",
            "Iteration 87: Total Cost = $0.0136 \n",
            "Iteration 90: Total Cost = $0.0141 \n",
            "Iteration 93: Total Cost = $0.0145 \n",
            "Iteration 96: Total Cost = $0.0150 \n",
            "Iteration 99: Total Cost = $0.0155 \n",
            "Iteration 102: Total Cost = $0.0160 \n",
            "Iteration 105: Total Cost = $0.0164 \n",
            "Iteration 108: Total Cost = $0.0169 \n",
            "Iteration 111: Total Cost = $0.0174 \n",
            "Iteration 114: Total Cost = $0.0178 \n",
            "Iteration 117: Total Cost = $0.0183 \n",
            "Iteration 120: Total Cost = $0.0188 \n",
            "Iteration 123: Total Cost = $0.0193 \n",
            "Iteration 126: Total Cost = $0.0197 \n",
            "Iteration 129: Total Cost = $0.0202 \n",
            "Iteration 132: Total Cost = $0.0207 \n",
            "Iteration 135: Total Cost = $0.0212 \n",
            "Iteration 138: Total Cost = $0.0216 \n",
            "Iteration 141: Total Cost = $0.0221 \n"
          ]
        }
      ],
      "source": [
        "\n",
        "# cloud_model_id = \"gpt-4o-mini\"\n",
        "price_per_1m_input_tokens = 0.15\n",
        "price_per_1m_output_tokens = 0.60\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "\n",
        "data_dir = \"/content/drive/MyDrive/finetunning\"\n",
        "save_to = join(data_dir, \"torj_sft.jsonl\")\n",
        "\n",
        "ix = 0\n",
        "coutner_api = 0\n",
        "for obj in tqdm(raw_data[:3000]):\n",
        "\n",
        "    for targeted_lang in [\"Arabic\"]:\n",
        "\n",
        "        translation(obj['word'], obj['text']) # function self made to pass the prompt to AI\n",
        "\n",
        "        response = client.chat.completions.create(\n",
        "                                messages= translation(obj['word'], obj['text']),\n",
        "                                model= gpt_model_name,\n",
        "                                temperature=0.2,\n",
        "                            )\n",
        "\n",
        "        if response.choices[0].finish_reason != \"stop\":\n",
        "            prompt_tokens += response.usage.prompt_tokens\n",
        "            continue\n",
        "\n",
        "        llm_response = response.choices[0].message.content\n",
        "        llm_resp_dict = parse_json(llm_response)\n",
        "\n",
        "        if not llm_resp_dict:\n",
        "            continue\n",
        "\n",
        "        with open(save_to, \"a\", encoding=\"utf8\") as dest:\n",
        "            dest.write(json.dumps({\n",
        "                \"id\": ix,\n",
        "                \"text\": obj['text'].strip(),\n",
        "                \"word\": obj['word'].strip(),\n",
        "\n",
        "                \"output_scheme\": json.dumps( Translation.model_json_schema(), ensure_ascii=False ),\n",
        "                \"task\": f\"You have to translate the the word provided with its context to {targeted_lang} into a JSON.\",\n",
        "\n",
        "                \"response\": llm_resp_dict,\n",
        "            }, ensure_ascii=False, default=str)  + \"\\n\" )\n",
        "\n",
        "        ix += 1\n",
        "        prompt_tokens += response.usage.prompt_tokens\n",
        "        completion_tokens += response.usage.completion_tokens\n",
        "\n",
        "        if(ix % 3) == 0:\n",
        "            cost_input = (prompt_tokens / 1_000_000) * price_per_1m_input_tokens\n",
        "            cost_output = (completion_tokens / 1_000_000) * price_per_1m_output_tokens\n",
        "            total_cost = cost_input + cost_output\n",
        "\n",
        "            print(f\"Iteration {ix}: Total Cost = ${total_cost:.4f} \")\n",
        "            if total_cost > 0.95:\n",
        "                if coutner_api < 3:\n",
        "                  change_api_key(coutner_api)\n",
        "                  coutner_api += 1\n",
        "                else :\n",
        "                  print(\"api key has not changed\")\n",
        "                  print(\"finished in index: \",raw_data.index(obj))\n",
        "                  break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/finetunning/torj_sft.jsonl\", \"r\", encoding=\"utf8\") as dest:\n",
        "  tt = dest.readline()\n",
        "ttt = json.loads(tt.strip())\n",
        "print(ttt.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h67nyeLB62lT",
        "outputId": "b424d8d0-4a41-4b9b-de78-f7bfc28daecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['id', 'text', 'word', 'output_scheme', 'task', 'response'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tUZ5nlfO4mI"
      },
      "outputs": [],
      "source": [
        "sft_data_path1 = \"/content/drive/MyDrive/finetunning/torj_sft.jsonl\"\n",
        "sft_data_path2 = \"/content/drive/MyDrive/finetunning/torj_sft2.jsonl\"\n",
        "\n",
        "llm_finetunning_data = []\n",
        "\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are a professional NLP data parser.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"Do not generate any introduction or conclusion.\"\n",
        "])\n",
        "for sft_data_path in [sft_data_path1,sft_data_path2]:\n",
        "  for line in open(sft_data_path):\n",
        "      if line.strip() == \"\":\n",
        "          continue\n",
        "\n",
        "      rec = json.loads(line.strip())\n",
        "\n",
        "      llm_finetunning_data.append({\n",
        "          \"system\": system_message,\n",
        "          \"instruction\": \"\\n\".join([\n",
        "              \"# text:\",\n",
        "              rec[\"text\"],\n",
        "              \"\",\n",
        "\n",
        "              \"# word:\",\n",
        "              rec[\"word\"],\n",
        "              \"\",\n",
        "\n",
        "              \"# Task:\",\n",
        "              rec[\"task\"],\n",
        "\n",
        "              \"# Output Scheme:\",\n",
        "              rec[\"output_scheme\"],\n",
        "              \"\",\n",
        "\n",
        "              \"# Output JSON:\",\n",
        "              \"```json\"\n",
        "\n",
        "          ]),\n",
        "          \"input\": \"\",\n",
        "          \"output\": \"\\n\".join([\n",
        "              \"```json\",\n",
        "              json.dumps(rec[\"response\"], ensure_ascii=False, default=str),\n",
        "              \"```\"\n",
        "          ]),\n",
        "          \"history\": []\n",
        "      })\n",
        "\n",
        "  random.Random(101).shuffle(llm_finetunning_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(llm_finetunning_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZcaM-kV8FxE",
        "outputId": "3ed9bcd4-bada-409c-89bf-85da13851c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3026"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_sz = 2900\n",
        "\n",
        "train_ds = llm_finetunning_data[:train_sample_sz]\n",
        "eval_ds = llm_finetunning_data[train_sample_sz:]\n",
        "\n",
        "#os.makedirs( , exist_ok=True)\n",
        "\n",
        "with open('/content/drive/MyDrive/finetunning/llamafactory-finetune-data/train.json', \"w\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "with open(\"/content/drive/MyDrive/finetunning/llamafactory-finetune-data/val.json\", \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)"
      ],
      "metadata": {
        "id": "Wbh-UZ648Hrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Configure LLaMA-Factory for the new datasets\n",
        "\n",
        "# # update /content/LLaMA-Factory/data/dataset_info.json and append\n",
        "# ```\n",
        "   \"turjuman_finetune_train\": {\n",
        "        \"file_name\": \"/content/drive/MyDrive/finetunning/llamafactory-finetune-data/train.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    },\n",
        "    \"turjuman_finetune_val\": {\n",
        "        \"file_name\": \"/content/drive/MyDrive/finetunning/llamafactory-finetune-data/val.json\",\n",
        "        \"columns\": {\n",
        "            \"prompt\": \"instruction\",\n",
        "            \"query\": \"input\",\n",
        "            \"response\": \"output\",\n",
        "            \"system\": \"system\",\n",
        "            \"history\": \"history\"\n",
        "        }\n",
        "    }\n",
        "# ```\n",
        "\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/apwbkni9\n",
        "# https://wandb.ai/mr-bakrianoo/llamafactory/runs/c5tf0q90"
      ],
      "metadata": {
        "id": "K6qWthA3C18_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "wandb.login(key='ad3313b0d503f5b8f133c84d7a66a3ed3ffe0a58')\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128,
          "referenced_widgets": [
            "ae91da5d7dbd44999c03ac40ff7705dc",
            "a3362e09ad10400b97986215bf8813a3",
            "bfb5392f2eac4b60a76d444b1e4e4270",
            "3110b463a06941648205f7cce85e2e82",
            "d444e852b1c744e0902367b67d0867a8",
            "3dd65a3f37294b329957a59f020aceee",
            "e51acee8687c4240a86fdbac34f37f2d",
            "150973d1666c463493b86329c04ee62c",
            "d4bee874eb72456fabbed6b5bb0ec43f",
            "94f1f9487e024c2fb6367d59205df37d",
            "7c6cba68c58a4e7e8ae6d358160d5bb8",
            "70fbdb5c9e904b07ad1dd83cc4e4aec0",
            "6ef95c78d0f24d2098536bc9ed5f206d",
            "f7902bbcd6944bb48a90e1b8916fb4ce",
            "425e62c367c04782abfb31a7be852779",
            "addf90a3bb734298b23f9f377c4cd788",
            "48ca2117758c4029a7e74bf683bebad4",
            "5b76c18a57ab4258ba9c7c749ca685a2",
            "0391850802474b91a11d920ce7c6f686",
            "9b1336fd9c084439977383df0bfa6ba1"
          ]
        },
        "id": "QIY6yY59FU06",
        "outputId": "6b098e11-6b15-49cc-9fe2-161a74571619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmohamed-mido-azzam\u001b[0m (\u001b[33mmohamed-mido-azzam-mansoura-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae91da5d7dbd44999c03ac40ff7705dc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/turjuman_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: turjuman_finetune_train\n",
        "eval_dataset: turjuman_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "# resume_from_checkpoint: /gdrive/MyDrive/youtube-resources/llm-finetuning/models/checkpoint-1500\n",
        "output_dir: /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/\n",
        "logging_steps: 10\n",
        "save_steps: 50\n",
        "plot_loss: true\n",
        "# overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: turjuman-finetune-llamafactory\n",
        "\n",
        "push_to_hub: true\n",
        "export_hub_model_id: \"MoAzzam1/turjuman-analyzer\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRzlPrwiCghs",
        "outputId": "a314e31c-515e-4279-d91d-3bcdb76aab95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/LLaMA-Factory/examples/train_lora/turjuman_finetune.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/turjuman_finetune.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfRU9GNTG5Gu",
        "outputId": "03098b8a-1058-4503-8ee2-8e0b3ff80ed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-29 03:17:23.657461: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745896643.680712    5416 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745896643.687950    5416 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-04-29 03:17:33] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "tokenizer_config.json: 100% 7.30k/7.30k [00:00<00:00, 40.3MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 10.7MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 8.44MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 21.9MB/s]\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:35,884 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:35,884 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:35,884 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:35,884 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:35,884 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:35,884 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:35,884 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-29 03:17:36,224 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 660/660 [00:00<00:00, 5.94MB/s]\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 03:17:36,801 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 03:17:36,803 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:36,888 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:36,888 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:36,888 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:36,888 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:36,889 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:36,889 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-29 03:17:36,889 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-29 03:17:37,228 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-29 03:17:37] llamafactory.data.template:143 >> Add <|im_end|> to stop words.\n",
            "[INFO|2025-04-29 03:17:37] llamafactory.data.loader:143 >> Loading dataset /content/drive/MyDrive/finetunning/llamafactory-finetune-data/train.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 2900 examples [00:00, 3482.64 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 2900/2900 [00:01<00:00, 2066.83 examples/s]\n",
            "[INFO|2025-04-29 03:17:41] llamafactory.data.loader:143 >> Loading dataset /content/drive/MyDrive/finetunning/llamafactory-finetune-data/val.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 126 examples [00:00, 5516.17 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 126/126 [00:00<00:00, 305.83 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 2900/2900 [00:17<00:00, 163.48 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 6584, 451, 12567, 821, 6729, 624, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 323, 279, 1565, 5097, 43781, 63, 311, 6923, 279, 1565, 5097, 4718, 18639, 5404, 537, 6923, 894, 16800, 476, 16688, 13, 151645, 198, 151644, 872, 198, 2, 1467, 510, 16169, 386, 14225, 12068, 11, 36977, 4463, 811, 4517, 11, 19685, 55706, 482, 11, 42482, 75300, 3165, 409, 1187, 4716, 924, 11, 444, 2792, 409, 64606, 11, 24986, 2876, 10303, 68, 11, 323, 7355, 12285, 930, 13, 5542, 279, 220, 16, 21, 339, 311, 279, 220, 16, 23, 339, 9294, 1198, 7399, 24237, 6, 19840, 44418, 92539, 304, 279, 14371, 315, 15344, 323, 9625, 13, 4329, 1198, 7399, 24237, 6, 19840, 11088, 1033, 5326, 1495, 13, 11733, 279, 5326, 11088, 323, 279, 79063, 367, 1033, 31449, 5193, 17206, 315, 279, 882, 11, 7945, 5193, 279, 975, 315, 386, 14225, 12068, 13, 41382, 23554, 5193, 279, 18560, 315, 13296, 388, 323, 357, 16210, 4217, 304, 271, 2, 3409, 510, 44344, 554, 271, 2, 5430, 510, 2610, 614, 311, 14683, 279, 279, 3409, 3897, 448, 1181, 2266, 311, 34117, 1119, 264, 4718, 624, 2, 9258, 43781, 510, 4913, 13193, 788, 5212, 53242, 13533, 788, 5212, 4684, 788, 330, 785, 24531, 3409, 304, 279, 2169, 4128, 11, 3118, 389, 279, 3897, 2266, 10465, 330, 60992, 788, 220, 17, 20, 20, 11, 330, 1065, 4373, 788, 220, 17, 11, 330, 2102, 788, 330, 81016, 9322, 497, 330, 1313, 788, 330, 917, 14345, 330, 5657, 51393, 45603, 788, 5212, 4684, 788, 330, 69123, 68493, 4244, 304, 34117, 11, 9760, 311, 279, 2266, 13, 2308, 42328, 10465, 330, 3615, 788, 5212, 1313, 788, 330, 917, 14345, 330, 2810, 4353, 788, 220, 20, 11, 330, 1065, 4353, 788, 220, 16, 11, 330, 2102, 788, 330, 6397, 23153, 45603, 497, 330, 1313, 788, 330, 1653, 14345, 330, 2427, 51393, 45603, 788, 5212, 4684, 788, 330, 37134, 45603, 315, 279, 3409, 304, 6364, 13, 2308, 42328, 10465, 330, 3615, 788, 5212, 1313, 788, 330, 917, 14345, 330, 2810, 4353, 788, 220, 20, 11, 330, 1065, 4353, 788, 220, 16, 11, 330, 2102, 788, 330, 3608, 23153, 45603, 497, 330, 1313, 788, 330, 1653, 14345, 330, 18375, 788, 5212, 4684, 788, 330, 10398, 315, 279, 4024, 3409, 304, 6364, 10465, 330, 60992, 788, 220, 17, 20, 20, 11, 330, 1065, 4373, 788, 220, 20, 11, 330, 2102, 788, 330, 10398, 497, 330, 1313, 788, 330, 917, 14345, 330, 8687, 31507, 788, 5212, 4684, 788, 330, 2082, 3110, 11652, 476, 17133, 1667, 279, 3409, 311, 19869, 1181, 10431, 304, 2266, 304, 6364, 10465, 330, 60992, 788, 220, 17, 20, 20, 11, 330, 1065, 4373, 788, 220, 20, 11, 330, 2102, 788, 330, 13314, 24567, 497, 330, 1313, 788, 330, 917, 9207, 2137, 330, 6279, 788, 4383, 53242, 13533, 497, 330, 5657, 51393, 45603, 497, 330, 2427, 51393, 45603, 497, 330, 18375, 497, 330, 8687, 31507, 7914, 330, 2102, 788, 330, 24412, 497, 330, 1313, 788, 330, 1700, 63159, 2, 9258, 4718, 510, 73594, 2236, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 4913, 53242, 13533, 788, 330, 10176, 129657, 93543, 497, 330, 5657, 51393, 45603, 788, 4383, 132072, 23364, 124537, 124668, 497, 330, 126456, 39423, 497, 330, 142302, 497, 330, 32790, 123862, 11071, 497, 330, 10176, 129883, 7914, 330, 2427, 51393, 45603, 788, 4383, 1363, 52752, 497, 330, 3094, 497, 330, 67, 2396, 266, 380, 497, 330, 5368, 295, 497, 330, 69795, 7914, 330, 18375, 788, 330, 32, 8585, 98368, 323, 12089, 879, 374, 6509, 825, 315, 279, 12196, 35367, 315, 22358, 304, 10867, 17206, 10465, 330, 8687, 31507, 788, 330, 44, 14225, 12068, 594, 11088, 3545, 7578, 404, 1506, 279, 3590, 39751, 315, 806, 882, 11, 3259, 1105, 2176, 29211, 323, 3381, 9838, 85, 10746, 1189, 532, 73594, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a professional NLP data parser.\n",
            "Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\n",
            "Do not generate any introduction or conclusion.<|im_end|>\n",
            "<|im_start|>user\n",
            "# text:\n",
            "including Molière, Pierre Corneille, Jean Racine, Pedro Calderón de la Barca, Lope de Vega, Christopher Marlowe, and Ben Jonson. From the 16th to the 18th century Commedia dell'arte performers improvised in the streets of Italy and France. Some Commedia dell'arte plays were written down. Both the written plays and the improvisation were influential upon literature of the time, particularly upon the work of Molière. Shakespeare drew upon the arts of jesters and strolling players in\n",
            "\n",
            "# word:\n",
            "molire\n",
            "\n",
            "# Task:\n",
            "You have to translate the the word provided with its context to Arabic into a JSON.\n",
            "# Output Scheme:\n",
            "{\"properties\": {\"translated_word\": {\"description\": \"The translated word in the target language, based on the provided context.\", \"maxLength\": 255, \"minLength\": 2, \"title\": \"Translated Word\", \"type\": \"string\"}, \"target_synonyms\": {\"description\": \"Different synonymous words in Arabic, relevant to the context. No duplicates.\", \"items\": {\"type\": \"string\"}, \"maxItems\": 5, \"minItems\": 1, \"title\": \"Target Synonyms\", \"type\": \"array\"}, \"source_synonyms\": {\"description\": \"Synonyms of the word in English. No duplicates.\", \"items\": {\"type\": \"string\"}, \"maxItems\": 5, \"minItems\": 1, \"title\": \"Source Synonyms\", \"type\": \"array\"}, \"definition\": {\"description\": \"Definition of the original word in English.\", \"maxLength\": 255, \"minLength\": 5, \"title\": \"Definition\", \"type\": \"string\"}, \"example_usage\": {\"description\": \"An example sentence or phrase using the word to demonstrate its usage in context in English.\", \"maxLength\": 255, \"minLength\": 5, \"title\": \"Example Usage\", \"type\": \"string\"}}, \"required\": [\"translated_word\", \"target_synonyms\", \"source_synonyms\", \"definition\", \"example_usage\"], \"title\": \"Translation\", \"type\": \"object\"}\n",
            "\n",
            "# Output JSON:\n",
            "```json<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\"translated_word\": \"موليير\", \"target_synonyms\": [\"كاتب مسرحي\", \"فنان\", \"مؤلف\", \"شاعر\", \"مخرج\"], \"source_synonyms\": [\"playwright\", \"author\", \"dramatist\", \"poet\", \"director\"], \"definition\": \"A French playwright and actor who is considered one of the greatest masters of comedy in Western literature.\", \"example_usage\": \"Molière's plays often satirized the social norms of his time, making them both entertaining and thought-provoking.\"}\n",
            "```<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 4913, 53242, 13533, 788, 330, 10176, 129657, 93543, 497, 330, 5657, 51393, 45603, 788, 4383, 132072, 23364, 124537, 124668, 497, 330, 126456, 39423, 497, 330, 142302, 497, 330, 32790, 123862, 11071, 497, 330, 10176, 129883, 7914, 330, 2427, 51393, 45603, 788, 4383, 1363, 52752, 497, 330, 3094, 497, 330, 67, 2396, 266, 380, 497, 330, 5368, 295, 497, 330, 69795, 7914, 330, 18375, 788, 330, 32, 8585, 98368, 323, 12089, 879, 374, 6509, 825, 315, 279, 12196, 35367, 315, 22358, 304, 10867, 17206, 10465, 330, 8687, 31507, 788, 330, 44, 14225, 12068, 594, 11088, 3545, 7578, 404, 1506, 279, 3590, 39751, 315, 806, 882, 11, 3259, 1105, 2176, 29211, 323, 3381, 9838, 85, 10746, 1189, 532, 73594, 151645, 198]\n",
            "labels:\n",
            "```json\n",
            "{\"translated_word\": \"موليير\", \"target_synonyms\": [\"كاتب مسرحي\", \"فنان\", \"مؤلف\", \"شاعر\", \"مخرج\"], \"source_synonyms\": [\"playwright\", \"author\", \"dramatist\", \"poet\", \"director\"], \"definition\": \"A French playwright and actor who is considered one of the greatest masters of comedy in Western literature.\", \"example_usage\": \"Molière's plays often satirized the social norms of his time, making them both entertaining and thought-provoking.\"}\n",
            "```<|im_end|>\n",
            "\n",
            "Running tokenizer on dataset (num_proc=16): 100% 126/126 [00:11<00:00, 11.09 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[151644, 8948, 198, 2610, 525, 264, 6584, 451, 12567, 821, 6729, 624, 12480, 279, 3897, 1565, 6262, 63, 553, 279, 1196, 323, 279, 1565, 5097, 43781, 63, 311, 6923, 279, 1565, 5097, 4718, 18639, 5404, 537, 6923, 894, 16800, 476, 16688, 13, 151645, 198, 151644, 872, 198, 2, 1467, 510, 46239, 389, 4128, 26, 807, 3000, 1576, 1052, 525, 7343, 429, 32051, 315, 1251, 4411, 13, 576, 1482, 36909, 315, 74059, 13230, 429, 458, 15235, 1410, 990, 4128, 311, 26910, 1251, 311, 4411, 4113, 11, 1496, 311, 1896, 6168, 429, 525, 39552, 624, 785, 17979, 23183, 11647, 323, 4958, 76714, 525, 9519, 11, 448, 78098, 64895, 2176, 11658, 323, 25062, 28544, 291, 553, 5214, 504, 41735, 2256, 396, 20509, 15235, 13, 19207, 1361, 1741, 438, 18095, 12611, 10566, 11, 8596, 35493, 11, 323, 68539, 39538, 11, 438, 1632, 438, 15235, 271, 2, 3409, 510, 29642, 908, 271, 2, 5430, 510, 2610, 614, 311, 14683, 279, 279, 3409, 3897, 448, 1181, 2266, 311, 34117, 1119, 264, 4718, 624, 2, 9258, 43781, 510, 4913, 13193, 788, 5212, 53242, 13533, 788, 5212, 4684, 788, 330, 785, 24531, 3409, 304, 279, 2169, 4128, 11, 3118, 389, 279, 3897, 2266, 10465, 330, 60992, 788, 220, 17, 20, 20, 11, 330, 1065, 4373, 788, 220, 17, 11, 330, 2102, 788, 330, 81016, 9322, 497, 330, 1313, 788, 330, 917, 14345, 330, 5657, 51393, 45603, 788, 5212, 4684, 788, 330, 69123, 68493, 4244, 304, 34117, 11, 9760, 311, 279, 2266, 13, 2308, 42328, 10465, 330, 3615, 788, 5212, 1313, 788, 330, 917, 14345, 330, 2810, 4353, 788, 220, 20, 11, 330, 1065, 4353, 788, 220, 16, 11, 330, 2102, 788, 330, 6397, 23153, 45603, 497, 330, 1313, 788, 330, 1653, 14345, 330, 2427, 51393, 45603, 788, 5212, 4684, 788, 330, 37134, 45603, 315, 279, 3409, 304, 6364, 13, 2308, 42328, 10465, 330, 3615, 788, 5212, 1313, 788, 330, 917, 14345, 330, 2810, 4353, 788, 220, 20, 11, 330, 1065, 4353, 788, 220, 16, 11, 330, 2102, 788, 330, 3608, 23153, 45603, 497, 330, 1313, 788, 330, 1653, 14345, 330, 18375, 788, 5212, 4684, 788, 330, 10398, 315, 279, 4024, 3409, 304, 6364, 10465, 330, 60992, 788, 220, 17, 20, 20, 11, 330, 1065, 4373, 788, 220, 20, 11, 330, 2102, 788, 330, 10398, 497, 330, 1313, 788, 330, 917, 14345, 330, 8687, 31507, 788, 5212, 4684, 788, 330, 2082, 3110, 11652, 476, 17133, 1667, 279, 3409, 311, 19869, 1181, 10431, 304, 2266, 304, 6364, 10465, 330, 60992, 788, 220, 17, 20, 20, 11, 330, 1065, 4373, 788, 220, 20, 11, 330, 2102, 788, 330, 13314, 24567, 497, 330, 1313, 788, 330, 917, 9207, 2137, 330, 6279, 788, 4383, 53242, 13533, 497, 330, 5657, 51393, 45603, 497, 330, 2427, 51393, 45603, 497, 330, 18375, 497, 330, 8687, 31507, 7914, 330, 2102, 788, 330, 24412, 497, 330, 1313, 788, 330, 1700, 63159, 2, 9258, 4718, 510, 73594, 2236, 151645, 198, 151644, 77091, 198, 73594, 2236, 198, 4913, 53242, 13533, 788, 330, 10176, 123897, 124669, 497, 330, 5657, 51393, 45603, 788, 4383, 124340, 125395, 53479, 80970, 126319, 497, 330, 10176, 80970, 126319, 497, 330, 130910, 129766, 497, 330, 31073, 84532, 14558, 132280, 497, 330, 130910, 55891, 125152, 7914, 330, 2427, 51393, 45603, 788, 4383, 25013, 908, 497, 330, 376, 90287, 497, 330, 1830, 1717, 497, 330, 2408, 46539, 497, 330, 75596, 782, 7914, 330, 18375, 788, 330, 32, 34776, 4647, 14064, 279, 1372, 220, 16, 11, 15, 15, 15, 11, 15, 15, 15, 11, 15, 15, 15, 476, 220, 16, 15, 61, 24, 10465, 330, 8687, 31507, 788, 330, 27476, 908, 315, 1251, 2163, 279, 1879, 17188, 389, 279, 7602, 369, 1995, 1189, 532, 73594, 151645, 198]\n",
            "inputs:\n",
            "<|im_start|>system\n",
            "You are a professional NLP data parser.\n",
            "Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\n",
            "Do not generate any introduction or conclusion.<|im_end|>\n",
            "<|im_start|>user\n",
            "# text:\n",
            "built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.\n",
            "The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI\n",
            "\n",
            "# word:\n",
            "billions\n",
            "\n",
            "# Task:\n",
            "You have to translate the the word provided with its context to Arabic into a JSON.\n",
            "# Output Scheme:\n",
            "{\"properties\": {\"translated_word\": {\"description\": \"The translated word in the target language, based on the provided context.\", \"maxLength\": 255, \"minLength\": 2, \"title\": \"Translated Word\", \"type\": \"string\"}, \"target_synonyms\": {\"description\": \"Different synonymous words in Arabic, relevant to the context. No duplicates.\", \"items\": {\"type\": \"string\"}, \"maxItems\": 5, \"minItems\": 1, \"title\": \"Target Synonyms\", \"type\": \"array\"}, \"source_synonyms\": {\"description\": \"Synonyms of the word in English. No duplicates.\", \"items\": {\"type\": \"string\"}, \"maxItems\": 5, \"minItems\": 1, \"title\": \"Source Synonyms\", \"type\": \"array\"}, \"definition\": {\"description\": \"Definition of the original word in English.\", \"maxLength\": 255, \"minLength\": 5, \"title\": \"Definition\", \"type\": \"string\"}, \"example_usage\": {\"description\": \"An example sentence or phrase using the word to demonstrate its usage in context in English.\", \"maxLength\": 255, \"minLength\": 5, \"title\": \"Example Usage\", \"type\": \"string\"}}, \"required\": [\"translated_word\", \"target_synonyms\", \"source_synonyms\", \"definition\", \"example_usage\"], \"title\": \"Translation\", \"type\": \"object\"}\n",
            "\n",
            "# Output JSON:\n",
            "```json<|im_end|>\n",
            "<|im_start|>assistant\n",
            "```json\n",
            "{\"translated_word\": \"مليارات\", \"target_synonyms\": [\"آلاف الملايين\", \"ملايين\", \"عدد كبير\", \"كثيرون\", \"عدد هائل\"], \"source_synonyms\": [\"millions\", \"trillions\", \"countless\", \"myriad\", \"numerous\"], \"definition\": \"A numerical term representing the number 1,000,000,000 or 10^9.\", \"example_usage\": \"Billions of people around the world rely on the internet for information.\"}\n",
            "```<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 73594, 2236, 198, 4913, 53242, 13533, 788, 330, 10176, 123897, 124669, 497, 330, 5657, 51393, 45603, 788, 4383, 124340, 125395, 53479, 80970, 126319, 497, 330, 10176, 80970, 126319, 497, 330, 130910, 129766, 497, 330, 31073, 84532, 14558, 132280, 497, 330, 130910, 55891, 125152, 7914, 330, 2427, 51393, 45603, 788, 4383, 25013, 908, 497, 330, 376, 90287, 497, 330, 1830, 1717, 497, 330, 2408, 46539, 497, 330, 75596, 782, 7914, 330, 18375, 788, 330, 32, 34776, 4647, 14064, 279, 1372, 220, 16, 11, 15, 15, 15, 11, 15, 15, 15, 11, 15, 15, 15, 476, 220, 16, 15, 61, 24, 10465, 330, 8687, 31507, 788, 330, 27476, 908, 315, 1251, 2163, 279, 1879, 17188, 389, 279, 7602, 369, 1995, 1189, 532, 73594, 151645, 198]\n",
            "labels:\n",
            "```json\n",
            "{\"translated_word\": \"مليارات\", \"target_synonyms\": [\"آلاف الملايين\", \"ملايين\", \"عدد كبير\", \"كثيرون\", \"عدد هائل\"], \"source_synonyms\": [\"millions\", \"trillions\", \"countless\", \"myriad\", \"numerous\"], \"definition\": \"A numerical term representing the number 1,000,000,000 or 10^9.\", \"example_usage\": \"Billions of people around the world rely on the internet for information.\"}\n",
            "```<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 03:18:13,846 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 03:18:13,847 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|2025-04-29 03:18:13] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "model.safetensors: 100% 3.09G/3.09G [00:14<00:00, 211MB/s] \n",
            "[INFO|modeling_utils.py:1124] 2025-04-29 03:18:28,938 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors\n",
            "[INFO|modeling_utils.py:2167] 2025-04-29 03:18:28,939 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1142] 2025-04-29 03:18:28,941 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:328] 2025-04-29 03:18:28,971 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "[INFO|modeling_utils.py:4930] 2025-04-29 03:18:30,570 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4938] 2025-04-29 03:18:30,571 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 242/242 [00:00<00:00, 2.01MB/s]\n",
            "[INFO|configuration_utils.py:1097] 2025-04-29 03:18:30,743 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json\n",
            "[INFO|configuration_utils.py:1142] 2025-04-29 03:18:30,743 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.1,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-04-29 03:18:30] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-04-29 03:18:30] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-29 03:18:30] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-04-29 03:18:30] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-04-29 03:18:30] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,up_proj,gate_proj,q_proj,down_proj,o_proj,v_proj\n",
            "[INFO|2025-04-29 03:18:32] llamafactory.model.loader:143 >> trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\n",
            "[INFO|trainer.py:748] 2025-04-29 03:18:32,819 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2414] 2025-04-29 03:18:33,257 >> ***** Running training *****\n",
            "[INFO|trainer.py:2415] 2025-04-29 03:18:33,257 >>   Num examples = 2,900\n",
            "[INFO|trainer.py:2416] 2025-04-29 03:18:33,257 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2417] 2025-04-29 03:18:33,257 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:2420] 2025-04-29 03:18:33,257 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:2421] 2025-04-29 03:18:33,257 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2422] 2025-04-29 03:18:33,257 >>   Total optimization steps = 2,175\n",
            "[INFO|trainer.py:2423] 2025-04-29 03:18:33,261 >>   Number of trainable parameters = 73,859,072\n",
            "[INFO|integration_utils.py:831] 2025-04-29 03:18:33,265 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (0.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (0.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (1.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (1.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (1.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (1.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (1.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (1.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (1.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (1.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (1.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (1.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (2.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (2.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (2.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (2.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (2.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (2.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (2.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (2.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (2.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (2.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (3.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (3.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (3.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (3.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (3.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (3.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (3.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (3.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (3.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (3.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (4.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (4.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (4.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (4.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (4.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (4.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (4.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (4.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (4.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (4.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (5.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (5.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (5.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (5.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (5.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (5.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (5.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (5.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (5.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (5.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (6.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (6.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (6.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (6.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (6.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (6.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (6.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (6.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (6.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (6.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (7.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (7.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (7.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (7.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (7.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (7.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (7.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (7.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (7.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (7.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (8.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (8.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (8.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (8.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (8.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (8.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (8.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (8.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (8.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (8.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (9.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (9.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (9.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m creating run (9.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m creating run (9.0s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m creating run (9.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m creating run (9.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m creating run (9.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m creating run (9.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m creating run (9.5s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m creating run (10s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250429_031843-wi8xybcd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mturjuman-finetune-llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/mohamed-mido-azzam-mansoura-university/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/mohamed-mido-azzam-mansoura-university/llamafactory/runs/wi8xybcd\u001b[0m\n",
            "{'loss': 1.055, 'grad_norm': 3.5223147869110107, 'learning_rate': 4.128440366972477e-06, 'epoch': 0.01}\n",
            "{'loss': 0.9461, 'grad_norm': 2.0862696170806885, 'learning_rate': 8.71559633027523e-06, 'epoch': 0.03}\n",
            "{'loss': 0.8869, 'grad_norm': 1.9256389141082764, 'learning_rate': 1.3302752293577984e-05, 'epoch': 0.04}\n",
            "{'loss': 0.8431, 'grad_norm': 1.8569968938827515, 'learning_rate': 1.7889908256880737e-05, 'epoch': 0.06}\n",
            "{'loss': 0.8008, 'grad_norm': 1.663223147392273, 'learning_rate': 2.2477064220183487e-05, 'epoch': 0.07}\n",
            "  2% 50/2175 [09:41<7:00:33, 11.87s/it][INFO|trainer.py:3984] 2025-04-29 03:28:39,783 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-50\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 03:28:40,018 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 03:28:40,019 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 03:28:45,873 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-50/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 03:28:45,881 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-50/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 03:28:52,774 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 03:28:53,107 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.7991, 'grad_norm': 1.6648778915405273, 'learning_rate': 2.7064220183486238e-05, 'epoch': 0.08}\n",
            "{'loss': 0.7849, 'grad_norm': 1.9527415037155151, 'learning_rate': 3.1651376146788995e-05, 'epoch': 0.1}\n",
            "{'loss': 0.7468, 'grad_norm': 1.720735788345337, 'learning_rate': 3.623853211009174e-05, 'epoch': 0.11}\n",
            "{'loss': 0.8216, 'grad_norm': 1.7047650814056396, 'learning_rate': 4.0825688073394495e-05, 'epoch': 0.12}\n",
            "{'loss': 0.7263, 'grad_norm': 1.9644783735275269, 'learning_rate': 4.541284403669725e-05, 'epoch': 0.14}\n",
            "  5% 100/2175 [19:44<6:45:48, 11.73s/it][INFO|trainer.py:4307] 2025-04-29 03:38:42,765 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 03:38:42,766 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 03:38:42,766 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.43it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:55,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:53,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:12<01:51,  1.01it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.01it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:45,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:46,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:43,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.00s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.01it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:37,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:35,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:30,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:31,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:29,  1.00s/it]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.01it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:25,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.00it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:17,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.02it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.02it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.03it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.02it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.02it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.02it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:07,  1.00it/s]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.02it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.02it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.02it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.00it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.02it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:49,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.02it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:26<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:27<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:28<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:28<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:29<00:33,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:30<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:28,  1.04it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.03it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.01it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.01it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.02it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.02it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.03it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.7503550052642822, 'eval_turjuman_finetune_val_runtime': 124.2277, 'eval_turjuman_finetune_val_samples_per_second': 1.014, 'eval_turjuman_finetune_val_steps_per_second': 1.014, 'epoch': 0.14}\n",
            "  5% 100/2175 [21:48<6:45:48, 11.73s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 03:40:46,999 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-100\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 03:40:47,384 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 03:40:47,385 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 03:40:48,837 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 03:40:48,842 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-100/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 03:40:57,314 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 03:40:57,321 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.7571, 'grad_norm': 1.6467254161834717, 'learning_rate': 5e-05, 'epoch': 0.15}\n",
            "{'loss': 0.7924, 'grad_norm': 1.7126624584197998, 'learning_rate': 5.458715596330275e-05, 'epoch': 0.17}\n",
            "{'loss': 0.7652, 'grad_norm': 1.8340706825256348, 'learning_rate': 5.917431192660551e-05, 'epoch': 0.18}\n",
            "{'loss': 0.7273, 'grad_norm': 1.5989924669265747, 'learning_rate': 6.376146788990826e-05, 'epoch': 0.19}\n",
            "{'loss': 0.7449, 'grad_norm': 1.4506436586380005, 'learning_rate': 6.834862385321101e-05, 'epoch': 0.21}\n",
            "  7% 150/2175 [31:51<6:35:46, 11.73s/it][INFO|trainer.py:3984] 2025-04-29 03:50:50,291 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-150\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 03:50:50,543 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 03:50:50,545 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 03:50:51,618 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-150/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 03:50:51,623 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-150/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 03:50:56,086 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 03:50:56,092 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.7317, 'grad_norm': 1.4894428253173828, 'learning_rate': 7.293577981651376e-05, 'epoch': 0.22}\n",
            "{'loss': 0.7399, 'grad_norm': 1.9474679231643677, 'learning_rate': 7.752293577981652e-05, 'epoch': 0.23}\n",
            "{'loss': 0.7534, 'grad_norm': 1.7406401634216309, 'learning_rate': 8.211009174311927e-05, 'epoch': 0.25}\n",
            "{'loss': 0.7246, 'grad_norm': 1.4119271039962769, 'learning_rate': 8.669724770642202e-05, 'epoch': 0.26}\n",
            "{'loss': 0.7079, 'grad_norm': 1.5350614786148071, 'learning_rate': 9.128440366972478e-05, 'epoch': 0.28}\n",
            "  9% 200/2175 [41:47<6:30:20, 11.86s/it][INFO|trainer.py:4307] 2025-04-29 04:00:46,450 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 04:00:46,451 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 04:00:46,451 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:55,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:53,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.01it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:45,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:46,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:43,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.00s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.00it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:38,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:35,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:31,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:31,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:29,  1.00s/it]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.00it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:26,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.00it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:17,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.02it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.02it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.02it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.02it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.02it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.02it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:08,  1.00s/it]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.02it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.01it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.01it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.00it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.02it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:48,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.02it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:26<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:27<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:28<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:28<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:29<00:32,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:30<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:28,  1.04it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.03it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.02it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.02it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.00it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.02it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.02it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.04it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.7277946472167969, 'eval_turjuman_finetune_val_runtime': 124.2237, 'eval_turjuman_finetune_val_samples_per_second': 1.014, 'eval_turjuman_finetune_val_steps_per_second': 1.014, 'epoch': 0.28}\n",
            "  9% 200/2175 [43:52<6:30:20, 11.86s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 04:02:50,681 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-200\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 04:02:50,938 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 04:02:50,940 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:02:52,207 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:02:52,212 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-200/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:02:57,955 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:02:57,961 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.7463, 'grad_norm': 1.4486613273620605, 'learning_rate': 9.587155963302753e-05, 'epoch': 0.29}\n",
            "{'loss': 0.712, 'grad_norm': 1.4240264892578125, 'learning_rate': 9.99999355744686e-05, 'epoch': 0.3}\n",
            "{'loss': 0.724, 'grad_norm': 1.4742755889892578, 'learning_rate': 9.999220471158896e-05, 'epoch': 0.32}\n",
            "{'loss': 0.727, 'grad_norm': 1.2781893014907837, 'learning_rate': 9.997159102518792e-05, 'epoch': 0.33}\n",
            "{'loss': 0.7248, 'grad_norm': 2.1303627490997314, 'learning_rate': 9.993809982734328e-05, 'epoch': 0.34}\n",
            " 11% 250/2175 [53:50<6:21:25, 11.89s/it][INFO|trainer.py:3984] 2025-04-29 04:12:49,461 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-250\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 04:12:49,680 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 04:12:49,681 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:12:57,393 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-250/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:12:57,398 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-250/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:13:03,351 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:13:03,845 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.7113, 'grad_norm': 1.435165524482727, 'learning_rate': 9.989173974862445e-05, 'epoch': 0.36}\n",
            "{'loss': 0.7162, 'grad_norm': 1.398911476135254, 'learning_rate': 9.983252273586828e-05, 'epoch': 0.37}\n",
            "{'loss': 0.7276, 'grad_norm': 1.4023780822753906, 'learning_rate': 9.976046404910037e-05, 'epoch': 0.39}\n",
            "{'loss': 0.6844, 'grad_norm': 1.1620604991912842, 'learning_rate': 9.967558225760267e-05, 'epoch': 0.4}\n",
            "{'loss': 0.7012, 'grad_norm': 1.2045068740844727, 'learning_rate': 9.957789923512825e-05, 'epoch': 0.41}\n",
            " 14% 300/2175 [1:03:57<6:08:53, 11.80s/it][INFO|trainer.py:4307] 2025-04-29 04:22:56,403 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 04:22:56,403 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 04:22:56,404 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:55,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:52,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.02it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:46,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:46,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:42,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.00s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.01it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:38,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:35,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:31,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:30,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:29,  1.00s/it]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.00it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:26,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:20,  1.00s/it]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:17,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.02it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.02it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.03it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.02it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.02it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.02it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:08,  1.00s/it]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.00it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:04,  1.01it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.02it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:01,  1.00s/it]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.01it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.02it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.00it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.01it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:49,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.02it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:26<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:27<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:28<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:29<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:30<00:33,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:30<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:28,  1.04it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.01it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.00it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.02it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.01it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.03it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.6841461062431335, 'eval_turjuman_finetune_val_runtime': 124.315, 'eval_turjuman_finetune_val_samples_per_second': 1.014, 'eval_turjuman_finetune_val_steps_per_second': 1.014, 'epoch': 0.41}\n",
            " 14% 300/2175 [1:06:02<6:08:53, 11.80s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 04:25:00,724 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-300\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 04:25:00,980 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 04:25:00,981 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:25:06,885 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:25:06,889 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-300/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:25:11,708 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:25:11,714 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.6857, 'grad_norm': 1.2708224058151245, 'learning_rate': 9.946744015426442e-05, 'epoch': 0.43}\n",
            "{'loss': 0.7499, 'grad_norm': 1.1869217157363892, 'learning_rate': 9.934423347994596e-05, 'epoch': 0.44}\n",
            "{'loss': 0.7103, 'grad_norm': 1.1642612218856812, 'learning_rate': 9.920831096211969e-05, 'epoch': 0.46}\n",
            "{'loss': 0.6282, 'grad_norm': 1.2484135627746582, 'learning_rate': 9.90597076275627e-05, 'epoch': 0.47}\n",
            "{'loss': 0.7, 'grad_norm': 1.428451418876648, 'learning_rate': 9.889846177085598e-05, 'epoch': 0.48}\n",
            " 16% 350/2175 [1:16:02<5:54:54, 11.67s/it][INFO|trainer.py:3984] 2025-04-29 04:35:00,855 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-350\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 04:35:01,131 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 04:35:01,133 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:35:06,354 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-350/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:35:06,360 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-350/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:35:13,811 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:35:13,817 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.7353, 'grad_norm': 1.2965694665908813, 'learning_rate': 9.872461494451615e-05, 'epoch': 0.5}\n",
            "{'loss': 0.6556, 'grad_norm': 1.2663317918777466, 'learning_rate': 9.85382119482874e-05, 'epoch': 0.51}\n",
            "{'loss': 0.6747, 'grad_norm': 1.3565963506698608, 'learning_rate': 9.833930081759682e-05, 'epoch': 0.52}\n",
            "{'loss': 0.6902, 'grad_norm': 1.2661765813827515, 'learning_rate': 9.81279328111758e-05, 'epoch': 0.54}\n",
            "{'loss': 0.6395, 'grad_norm': 1.3431980609893799, 'learning_rate': 9.790416239785085e-05, 'epoch': 0.55}\n",
            " 18% 400/2175 [1:26:04<5:52:49, 11.93s/it][INFO|trainer.py:4307] 2025-04-29 04:45:02,751 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 04:45:02,751 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 04:45:02,751 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:55,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:53,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.01it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.01it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.01it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:47,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:46,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:45,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:47,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:45,  1.00s/it]\u001b[A\n",
            " 17% 22/126 [00:20<01:43,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.01s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.01it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:38,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:36,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.01it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:31,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:31,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:29,  1.00s/it]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.00it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:26,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:20,  1.00s/it]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:17,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.02it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.02it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.03it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.02it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.02it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.02it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:08,  1.00s/it]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.02it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.01it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.01it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.00it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.01it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:49,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.01s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.01it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:26<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:27<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:28<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:29<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:30<00:33,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:31<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:28,  1.03it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.01it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.01it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.02s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.01s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.00it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.01it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.01it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.03it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.6811760663986206, 'eval_turjuman_finetune_val_runtime': 124.4159, 'eval_turjuman_finetune_val_samples_per_second': 1.013, 'eval_turjuman_finetune_val_steps_per_second': 1.013, 'epoch': 0.55}\n",
            " 18% 400/2175 [1:28:08<5:52:49, 11.93s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 04:47:07,173 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-400\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 04:47:07,408 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 04:47:07,409 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:47:13,992 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:47:13,998 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-400/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:47:18,184 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:47:18,191 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.7152, 'grad_norm': 1.1587072610855103, 'learning_rate': 9.766804724250715e-05, 'epoch': 0.57}\n",
            "{'loss': 0.7246, 'grad_norm': 1.4068433046340942, 'learning_rate': 9.741964819122846e-05, 'epoch': 0.58}\n",
            "{'loss': 0.6782, 'grad_norm': 1.351178765296936, 'learning_rate': 9.715902925561742e-05, 'epoch': 0.59}\n",
            "{'loss': 0.6651, 'grad_norm': 1.398900032043457, 'learning_rate': 9.68862575962998e-05, 'epoch': 0.61}\n",
            "{'loss': 0.6921, 'grad_norm': 1.2461003065109253, 'learning_rate': 9.660140350561754e-05, 'epoch': 0.62}\n",
            " 21% 450/2175 [1:38:08<5:39:39, 11.81s/it][INFO|trainer.py:3984] 2025-04-29 04:57:07,054 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-450\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 04:57:07,324 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 04:57:07,325 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:57:08,233 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-450/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:57:08,240 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-450/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 04:57:16,091 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 04:57:16,104 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.6276, 'grad_norm': 1.4101065397262573, 'learning_rate': 9.630454038951468e-05, 'epoch': 0.63}\n",
            "{'loss': 0.6228, 'grad_norm': 1.0296878814697266, 'learning_rate': 9.599574474862078e-05, 'epoch': 0.65}\n",
            "{'loss': 0.6273, 'grad_norm': 1.2810516357421875, 'learning_rate': 9.567509615853707e-05, 'epoch': 0.66}\n",
            "{'loss': 0.648, 'grad_norm': 1.1923513412475586, 'learning_rate': 9.534267724933002e-05, 'epoch': 0.68}\n",
            "{'loss': 0.6517, 'grad_norm': 1.1420345306396484, 'learning_rate': 9.499857368423785e-05, 'epoch': 0.69}\n",
            " 23% 500/2175 [1:48:09<5:35:03, 12.00s/it][INFO|trainer.py:4307] 2025-04-29 05:07:08,151 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 05:07:08,151 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 05:07:08,151 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:54,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:52,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.02it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:45,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:46,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:44,  1.00s/it]\u001b[A\n",
            " 18% 23/126 [00:21<01:45,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.01s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.00it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:38,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:35,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:30,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:30,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:28,  1.00it/s]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.01it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:25,  1.02it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:22,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.00it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:16,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.02it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.02it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.03it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.02it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.03it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.03it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:07,  1.00it/s]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.02it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.01it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.02it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.01it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.02it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:48,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.01it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.02it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.02it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:25<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:26<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:27<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:28<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:29<00:33,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:30<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.06it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:28,  1.04it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.02it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.01it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.02it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.02it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.02it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.02it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.05it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.04it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.6662774682044983, 'eval_turjuman_finetune_val_runtime': 124.0715, 'eval_turjuman_finetune_val_samples_per_second': 1.016, 'eval_turjuman_finetune_val_steps_per_second': 1.016, 'epoch': 0.69}\n",
            " 23% 500/2175 [1:50:13<5:35:03, 12.00s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 05:09:12,228 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-500\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 05:09:12,473 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 05:09:12,474 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:09:16,501 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:09:16,507 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:09:27,242 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:09:27,258 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.6695, 'grad_norm': 1.1838871240615845, 'learning_rate': 9.464287413759547e-05, 'epoch': 0.7}\n",
            "{'loss': 0.6243, 'grad_norm': 1.403643250465393, 'learning_rate': 9.42756702719833e-05, 'epoch': 0.72}\n",
            "{'loss': 0.6988, 'grad_norm': 1.256574273109436, 'learning_rate': 9.389705671460625e-05, 'epoch': 0.73}\n",
            "{'loss': 0.6354, 'grad_norm': 1.3508833646774292, 'learning_rate': 9.350713103290847e-05, 'epoch': 0.74}\n",
            "{'loss': 0.6208, 'grad_norm': 1.2374402284622192, 'learning_rate': 9.310599370943063e-05, 'epoch': 0.76}\n",
            " 25% 550/2175 [2:00:17<5:18:34, 11.76s/it][INFO|trainer.py:3984] 2025-04-29 05:19:16,178 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-550\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 05:19:16,401 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 05:19:16,402 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:19:20,541 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-550/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:19:20,545 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-550/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:19:26,541 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:19:26,653 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.6225, 'grad_norm': 1.2329715490341187, 'learning_rate': 9.269374811591594e-05, 'epoch': 0.77}\n",
            "{'loss': 0.6695, 'grad_norm': 1.0613224506378174, 'learning_rate': 9.227050048667147e-05, 'epoch': 0.79}\n",
            "{'loss': 0.6491, 'grad_norm': 1.1079636812210083, 'learning_rate': 9.18363598911921e-05, 'epoch': 0.8}\n",
            "{'loss': 0.6874, 'grad_norm': 1.0658972263336182, 'learning_rate': 9.139143820605351e-05, 'epoch': 0.81}\n",
            "{'loss': 0.6131, 'grad_norm': 1.197069764137268, 'learning_rate': 9.093585008608208e-05, 'epoch': 0.83}\n",
            " 28% 600/2175 [2:10:20<5:08:36, 11.76s/it][INFO|trainer.py:4307] 2025-04-29 05:29:19,039 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 05:29:19,039 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 05:29:19,040 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:55,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:53,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.01it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.01it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:46,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:46,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:43,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.01s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.01it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:38,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:36,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:31,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:30,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:28,  1.00it/s]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.01it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:25,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.00it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:17,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.02it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.02it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.02it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.02it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.02it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.02it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:08,  1.00s/it]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.02it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.01it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.01it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.01s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:55,  1.00s/it]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.00it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.01it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:49,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.01s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.01it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:26<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:27<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:28<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:29<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:30<00:33,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:31<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:28,  1.03it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.02it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.00it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.01it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.01it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.03it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.648256242275238, 'eval_turjuman_finetune_val_runtime': 124.3352, 'eval_turjuman_finetune_val_samples_per_second': 1.013, 'eval_turjuman_finetune_val_steps_per_second': 1.013, 'epoch': 0.83}\n",
            " 28% 600/2175 [2:12:24<5:08:36, 11.76s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 05:31:23,380 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-600\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 05:31:23,655 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 05:31:23,657 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:31:24,975 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:31:24,984 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-600/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:31:32,710 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:31:32,717 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.6001, 'grad_norm': 1.206794023513794, 'learning_rate': 9.046971293480863e-05, 'epoch': 0.84}\n",
            "{'loss': 0.5976, 'grad_norm': 0.9492512345314026, 'learning_rate': 8.999314687421401e-05, 'epoch': 0.86}\n",
            "{'loss': 0.6543, 'grad_norm': 1.4670847654342651, 'learning_rate': 8.950627471377402e-05, 'epoch': 0.87}\n",
            "{'loss': 0.5713, 'grad_norm': 0.986383318901062, 'learning_rate': 8.900922191881186e-05, 'epoch': 0.88}\n",
            "{'loss': 0.6382, 'grad_norm': 1.1417227983474731, 'learning_rate': 8.850211657816607e-05, 'epoch': 0.9}\n",
            " 30% 650/2175 [2:22:25<4:57:06, 11.69s/it][INFO|trainer.py:3984] 2025-04-29 05:41:24,494 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-650\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 05:41:24,763 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 05:41:24,765 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:41:25,817 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-650/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:41:26,470 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-650/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:41:33,012 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:41:33,192 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.6884, 'grad_norm': 1.0795016288757324, 'learning_rate': 8.798508937118251e-05, 'epoch': 0.91}\n",
            "{'loss': 0.6308, 'grad_norm': 1.304499626159668, 'learning_rate': 8.745827353403875e-05, 'epoch': 0.92}\n",
            "{'loss': 0.6623, 'grad_norm': 1.3457943201065063, 'learning_rate': 8.692180482540951e-05, 'epoch': 0.94}\n",
            "{'loss': 0.6625, 'grad_norm': 1.1059166193008423, 'learning_rate': 8.637582149148215e-05, 'epoch': 0.95}\n",
            "{'loss': 0.625, 'grad_norm': 1.1410303115844727, 'learning_rate': 8.5820464230331e-05, 'epoch': 0.97}\n",
            " 32% 700/2175 [2:32:24<4:47:37, 11.70s/it][INFO|trainer.py:4307] 2025-04-29 05:51:23,232 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 05:51:23,232 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 05:51:23,233 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.03it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:04<01:51,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.03it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:54,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:09<01:52,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.02it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:47,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:45,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:46,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.01it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:42,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.01s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.00s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.01it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:37,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:35,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:32,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:31,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:30,  1.03it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:30,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:28,  1.00it/s]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.01it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:25,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.01it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:16,  1.02it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.03it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.03it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.03it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.03it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.03it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.03it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:07,  1.00it/s]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.03it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.01it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.02it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.00it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.02it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:48,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.02it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:23<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:24<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:25<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:26<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:27<00:34,  1.04it/s]\u001b[A\n",
            " 72% 91/126 [01:28<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:29<00:32,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:30<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:29,  1.03it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.02it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.00it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.02it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.02it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.01it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.03it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.6401174664497375, 'eval_turjuman_finetune_val_runtime': 124.069, 'eval_turjuman_finetune_val_samples_per_second': 1.016, 'eval_turjuman_finetune_val_steps_per_second': 1.016, 'epoch': 0.97}\n",
            " 32% 700/2175 [2:34:28<4:47:37, 11.70s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 05:53:27,307 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-700\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 05:53:27,528 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 05:53:27,529 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:53:32,013 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-700/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:53:32,017 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-700/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 05:53:37,462 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 05:53:37,467 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.6191, 'grad_norm': 1.2695903778076172, 'learning_rate': 8.525587615566009e-05, 'epoch': 0.98}\n",
            "{'loss': 0.6433, 'grad_norm': 1.0576255321502686, 'learning_rate': 8.468220275992307e-05, 'epoch': 0.99}\n",
            "{'loss': 0.5484, 'grad_norm': 0.9759861826896667, 'learning_rate': 8.409959187683039e-05, 'epoch': 1.01}\n",
            "{'loss': 0.4362, 'grad_norm': 1.1847327947616577, 'learning_rate': 8.350819364325306e-05, 'epoch': 1.02}\n",
            "{'loss': 0.4353, 'grad_norm': 1.234063982963562, 'learning_rate': 8.290816046053279e-05, 'epoch': 1.03}\n",
            " 34% 750/2175 [2:44:29<4:41:58, 11.87s/it][INFO|trainer.py:3984] 2025-04-29 06:03:28,061 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-750\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 06:03:28,582 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 06:03:28,583 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:03:34,240 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-750/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:03:34,246 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-750/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:03:46,768 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:03:46,782 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.4517, 'grad_norm': 1.1166647672653198, 'learning_rate': 8.229964695520873e-05, 'epoch': 1.05}\n",
            "{'loss': 0.438, 'grad_norm': 1.457658052444458, 'learning_rate': 8.168280993917077e-05, 'epoch': 1.06}\n",
            "{'loss': 0.4524, 'grad_norm': 1.3017016649246216, 'learning_rate': 8.105780836924957e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4244, 'grad_norm': 1.0366829633712769, 'learning_rate': 8.042480330625397e-05, 'epoch': 1.09}\n",
            "{'loss': 0.437, 'grad_norm': 1.093278408050537, 'learning_rate': 7.978395787346611e-05, 'epoch': 1.1}\n",
            " 37% 800/2175 [2:54:38<4:34:07, 11.96s/it][INFO|trainer.py:4307] 2025-04-29 06:13:37,100 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 06:13:37,100 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 06:13:37,100 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.43it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:55,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:53,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.01it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:46,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:47,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:43,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.00s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.01it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:37,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:35,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:30,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:30,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:28,  1.00it/s]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.01it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:25,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.00it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:16,  1.02it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.03it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.03it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:11,  1.03it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.03it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.02it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.03it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:07,  1.00it/s]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.03it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.02it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.02it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.01it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.01it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:48,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.02it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.02it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:25<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:26<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:27<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:28<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:29<00:33,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:30<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:28,  1.04it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.02it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.01it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.02it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.01it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.04it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.6547836661338806, 'eval_turjuman_finetune_val_runtime': 124.1373, 'eval_turjuman_finetune_val_samples_per_second': 1.015, 'eval_turjuman_finetune_val_steps_per_second': 1.015, 'epoch': 1.1}\n",
            " 37% 800/2175 [2:56:42<4:34:07, 11.96s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 06:15:41,245 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-800\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 06:15:41,476 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 06:15:41,478 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:15:42,772 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-800/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:15:42,777 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-800/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:15:48,174 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:15:48,182 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.4113, 'grad_norm': 1.2814823389053345, 'learning_rate': 7.913543721460516e-05, 'epoch': 1.12}\n",
            "{'loss': 0.4251, 'grad_norm': 1.1442688703536987, 'learning_rate': 7.84794084512703e-05, 'epoch': 1.13}\n",
            "{'loss': 0.4464, 'grad_norm': 1.1859303712844849, 'learning_rate': 7.781604063987406e-05, 'epoch': 1.14}\n",
            "{'loss': 0.4124, 'grad_norm': 1.3298085927963257, 'learning_rate': 7.714550472807706e-05, 'epoch': 1.16}\n",
            "{'loss': 0.3849, 'grad_norm': 1.138975977897644, 'learning_rate': 7.646797351073531e-05, 'epoch': 1.17}\n",
            " 39% 850/2175 [3:06:42<4:21:02, 11.82s/it][INFO|trainer.py:3984] 2025-04-29 06:25:40,984 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-850\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 06:25:41,223 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 06:25:41,224 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:25:48,366 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-850/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:25:48,372 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-850/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:25:53,079 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:25:53,087 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.4142, 'grad_norm': 1.0106110572814941, 'learning_rate': 7.578362158537169e-05, 'epoch': 1.19}\n",
            "{'loss': 0.4142, 'grad_norm': 1.182868242263794, 'learning_rate': 7.509262530718256e-05, 'epoch': 1.2}\n",
            "{'loss': 0.4169, 'grad_norm': 1.4720526933670044, 'learning_rate': 7.43951627435918e-05, 'epoch': 1.21}\n",
            "{'loss': 0.4199, 'grad_norm': 1.196175456047058, 'learning_rate': 7.369141362836334e-05, 'epoch': 1.23}\n",
            "{'loss': 0.4412, 'grad_norm': 0.9264469146728516, 'learning_rate': 7.298155931528437e-05, 'epoch': 1.24}\n",
            " 41% 900/2175 [3:16:44<4:09:46, 11.75s/it][INFO|trainer.py:4307] 2025-04-29 06:35:43,596 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 06:35:43,596 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 06:35:43,596 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.03it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:55,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:52,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:51,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.02it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:46,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:45,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:47,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:43,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.01s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.00it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:38,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:36,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.01it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:31,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:31,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:28,  1.00it/s]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.00it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:26,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.00it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:17,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.02it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.02it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.02it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.02it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.02it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.02it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:08,  1.00s/it]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.02it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.01it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.02it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.01s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.00it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.01it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.01it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:49,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.01it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:26<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:27<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:28<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:29<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:30<00:33,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:31<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:29,  1.03it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.01it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.01it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.01it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.01it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.01it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.03it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                          \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.6394799947738647, 'eval_turjuman_finetune_val_runtime': 124.3456, 'eval_turjuman_finetune_val_samples_per_second': 1.013, 'eval_turjuman_finetune_val_steps_per_second': 1.013, 'epoch': 1.24}\n",
            " 41% 900/2175 [3:18:49<4:09:46, 11.75s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 06:37:47,948 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-900\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 06:37:48,160 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 06:37:48,161 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:37:50,874 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-900/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:37:50,878 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-900/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:37:58,238 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:37:58,247 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.4623, 'grad_norm': 1.3775310516357422, 'learning_rate': 7.226578273143108e-05, 'epoch': 1.26}\n",
            "{'loss': 0.3778, 'grad_norm': 1.1886115074157715, 'learning_rate': 7.154426833002903e-05, 'epoch': 1.27}\n",
            "{'loss': 0.419, 'grad_norm': 1.258710503578186, 'learning_rate': 7.08172020429201e-05, 'epoch': 1.28}\n",
            "{'loss': 0.4181, 'grad_norm': 1.204925775527954, 'learning_rate': 7.008477123264848e-05, 'epoch': 1.3}\n",
            "{'loss': 0.415, 'grad_norm': 1.2550724744796753, 'learning_rate': 6.934716464417801e-05, 'epoch': 1.31}\n",
            " 44% 950/2175 [3:28:48<4:02:11, 11.86s/it][INFO|trainer.py:3984] 2025-04-29 06:47:47,601 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-950\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 06:47:47,865 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 06:47:47,867 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:47:49,149 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-950/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:47:49,155 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-950/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:47:55,956 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:47:55,967 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.4592, 'grad_norm': 1.4460304975509644, 'learning_rate': 6.860457235625322e-05, 'epoch': 1.32}\n",
            "{'loss': 0.4194, 'grad_norm': 1.3084477186203003, 'learning_rate': 6.785718573241665e-05, 'epoch': 1.34}\n",
            "{'loss': 0.3972, 'grad_norm': 1.4080829620361328, 'learning_rate': 6.710519737169515e-05, 'epoch': 1.35}\n",
            "{'loss': 0.3688, 'grad_norm': 1.5157346725463867, 'learning_rate': 6.634880105896773e-05, 'epoch': 1.37}\n",
            "{'loss': 0.3933, 'grad_norm': 1.0462788343429565, 'learning_rate': 6.558819171502786e-05, 'epoch': 1.38}\n",
            " 46% 1000/2175 [3:38:48<3:52:54, 11.89s/it][INFO|trainer.py:4307] 2025-04-29 06:57:46,917 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 06:57:46,918 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 06:57:46,918 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:54,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:52,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:50,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.02it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:45,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:46,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:42,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.01s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.00s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.01it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:37,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:35,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:30,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:30,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:28,  1.00it/s]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.01it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:25,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.00it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:16,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.03it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.03it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:11,  1.03it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.03it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.02it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.02it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:07,  1.00it/s]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.03it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.02it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.02it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.01it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.02it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:48,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.02it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:25<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:26<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:27<00:34,  1.04it/s]\u001b[A\n",
            " 72% 91/126 [01:28<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:29<00:32,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:30<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:28,  1.03it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.01it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.00it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.01it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.01it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.03it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.6461113095283508, 'eval_turjuman_finetune_val_runtime': 124.1062, 'eval_turjuman_finetune_val_samples_per_second': 1.015, 'eval_turjuman_finetune_val_steps_per_second': 1.015, 'epoch': 1.38}\n",
            " 46% 1000/2175 [3:40:52<3:52:54, 11.89s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 06:59:51,030 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1000\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 06:59:51,384 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 06:59:51,385 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:59:52,878 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:59:52,883 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 06:59:58,495 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 06:59:58,504 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.3918, 'grad_norm': 1.6117335557937622, 'learning_rate': 6.482356534635298e-05, 'epoch': 1.39}\n",
            "{'loss': 0.3866, 'grad_norm': 1.2451211214065552, 'learning_rate': 6.40551189945944e-05, 'epoch': 1.41}\n",
            "{'loss': 0.3968, 'grad_norm': 1.2066022157669067, 'learning_rate': 6.328305068580024e-05, 'epoch': 1.42}\n",
            "{'loss': 0.402, 'grad_norm': 1.2822024822235107, 'learning_rate': 6.250755937938475e-05, 'epoch': 1.43}\n",
            "{'loss': 0.4224, 'grad_norm': 1.4868634939193726, 'learning_rate': 6.172884491685729e-05, 'epoch': 1.45}\n",
            " 48% 1050/2175 [3:50:49<3:39:24, 11.70s/it][INFO|trainer.py:3984] 2025-04-29 07:09:47,904 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1050\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 07:09:48,186 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 07:09:48,187 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 07:09:49,138 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1050/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 07:09:49,143 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1050/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 07:09:57,515 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 07:09:57,525 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.4341, 'grad_norm': 1.1762793064117432, 'learning_rate': 6.0947107970323645e-05, 'epoch': 1.46}\n",
            "{'loss': 0.4098, 'grad_norm': 1.2607204914093018, 'learning_rate': 6.0162549990773645e-05, 'epoch': 1.48}\n",
            "{'loss': 0.3902, 'grad_norm': 1.2145837545394897, 'learning_rate': 5.9375373156167844e-05, 'epoch': 1.49}\n",
            "{'loss': 0.4518, 'grad_norm': 1.5967633724212646, 'learning_rate': 5.8585780319336994e-05, 'epoch': 1.5}\n",
            "{'loss': 0.4308, 'grad_norm': 1.1843562126159668, 'learning_rate': 5.7793974955707496e-05, 'epoch': 1.52}\n",
            " 51% 1100/2175 [4:00:47<3:29:37, 11.70s/it][INFO|trainer.py:4307] 2025-04-29 07:19:46,021 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4309] 2025-04-29 07:19:46,022 >>   Num examples = 126\n",
            "[INFO|trainer.py:4312] 2025-04-29 07:19:46,022 >>   Batch size = 1\n",
            "\n",
            "  0% 0/126 [00:00<?, ?it/s]\u001b[A\n",
            "  2% 2/126 [00:00<01:01,  2.02it/s]\u001b[A\n",
            "  2% 3/126 [00:01<01:25,  1.44it/s]\u001b[A\n",
            "  3% 4/126 [00:02<01:38,  1.24it/s]\u001b[A\n",
            "  4% 5/126 [00:03<01:44,  1.16it/s]\u001b[A\n",
            "  5% 6/126 [00:05<01:52,  1.07it/s]\u001b[A\n",
            "  6% 7/126 [00:06<01:56,  1.02it/s]\u001b[A\n",
            "  6% 8/126 [00:07<01:55,  1.02it/s]\u001b[A\n",
            "  7% 9/126 [00:08<01:54,  1.02it/s]\u001b[A\n",
            "  8% 10/126 [00:09<01:53,  1.02it/s]\u001b[A\n",
            "  9% 11/126 [00:10<01:52,  1.02it/s]\u001b[A\n",
            " 10% 12/126 [00:10<01:51,  1.02it/s]\u001b[A\n",
            " 10% 13/126 [00:11<01:50,  1.02it/s]\u001b[A\n",
            " 11% 14/126 [00:12<01:50,  1.02it/s]\u001b[A\n",
            " 12% 15/126 [00:13<01:49,  1.02it/s]\u001b[A\n",
            " 13% 16/126 [00:14<01:48,  1.02it/s]\u001b[A\n",
            " 13% 17/126 [00:15<01:46,  1.02it/s]\u001b[A\n",
            " 14% 18/126 [00:16<01:45,  1.02it/s]\u001b[A\n",
            " 15% 19/126 [00:17<01:44,  1.02it/s]\u001b[A\n",
            " 16% 20/126 [00:18<01:46,  1.01s/it]\u001b[A\n",
            " 17% 21/126 [00:19<01:44,  1.00it/s]\u001b[A\n",
            " 17% 22/126 [00:20<01:43,  1.01it/s]\u001b[A\n",
            " 18% 23/126 [00:21<01:44,  1.02s/it]\u001b[A\n",
            " 19% 24/126 [00:22<01:42,  1.00s/it]\u001b[A\n",
            " 20% 25/126 [00:23<01:40,  1.01it/s]\u001b[A\n",
            " 21% 26/126 [00:24<01:39,  1.01it/s]\u001b[A\n",
            " 21% 27/126 [00:25<01:37,  1.01it/s]\u001b[A\n",
            " 22% 28/126 [00:26<01:36,  1.01it/s]\u001b[A\n",
            " 23% 29/126 [00:27<01:35,  1.01it/s]\u001b[A\n",
            " 24% 30/126 [00:28<01:34,  1.02it/s]\u001b[A\n",
            " 25% 31/126 [00:29<01:33,  1.02it/s]\u001b[A\n",
            " 25% 32/126 [00:30<01:32,  1.02it/s]\u001b[A\n",
            " 26% 33/126 [00:31<01:30,  1.02it/s]\u001b[A\n",
            " 27% 34/126 [00:32<01:30,  1.02it/s]\u001b[A\n",
            " 28% 35/126 [00:33<01:29,  1.02it/s]\u001b[A\n",
            " 29% 36/126 [00:34<01:30,  1.01s/it]\u001b[A\n",
            " 29% 37/126 [00:35<01:28,  1.00it/s]\u001b[A\n",
            " 30% 38/126 [00:36<01:27,  1.01it/s]\u001b[A\n",
            " 31% 39/126 [00:37<01:25,  1.01it/s]\u001b[A\n",
            " 32% 40/126 [00:38<01:24,  1.02it/s]\u001b[A\n",
            " 33% 41/126 [00:39<01:23,  1.02it/s]\u001b[A\n",
            " 33% 42/126 [00:40<01:22,  1.02it/s]\u001b[A\n",
            " 34% 43/126 [00:41<01:21,  1.02it/s]\u001b[A\n",
            " 35% 44/126 [00:42<01:20,  1.02it/s]\u001b[A\n",
            " 36% 45/126 [00:43<01:21,  1.01s/it]\u001b[A\n",
            " 37% 46/126 [00:44<01:19,  1.00it/s]\u001b[A\n",
            " 37% 47/126 [00:45<01:18,  1.01it/s]\u001b[A\n",
            " 38% 48/126 [00:46<01:17,  1.01it/s]\u001b[A\n",
            " 39% 49/126 [00:47<01:15,  1.02it/s]\u001b[A\n",
            " 40% 50/126 [00:48<01:14,  1.02it/s]\u001b[A\n",
            " 40% 51/126 [00:49<01:13,  1.02it/s]\u001b[A\n",
            " 41% 52/126 [00:50<01:12,  1.03it/s]\u001b[A\n",
            " 42% 53/126 [00:51<01:11,  1.02it/s]\u001b[A\n",
            " 43% 54/126 [00:52<01:10,  1.03it/s]\u001b[A\n",
            " 44% 55/126 [00:53<01:09,  1.03it/s]\u001b[A\n",
            " 44% 56/126 [00:54<01:08,  1.02it/s]\u001b[A\n",
            " 45% 57/126 [00:55<01:09,  1.01s/it]\u001b[A\n",
            " 46% 58/126 [00:56<01:07,  1.00it/s]\u001b[A\n",
            " 47% 59/126 [00:57<01:06,  1.01it/s]\u001b[A\n",
            " 48% 60/126 [00:58<01:05,  1.01it/s]\u001b[A\n",
            " 48% 61/126 [00:59<01:03,  1.02it/s]\u001b[A\n",
            " 49% 62/126 [01:00<01:02,  1.02it/s]\u001b[A\n",
            " 50% 63/126 [01:01<01:01,  1.02it/s]\u001b[A\n",
            " 51% 64/126 [01:02<01:02,  1.01s/it]\u001b[A\n",
            " 52% 65/126 [01:03<01:00,  1.00it/s]\u001b[A\n",
            " 52% 66/126 [01:04<00:59,  1.01it/s]\u001b[A\n",
            " 53% 67/126 [01:05<00:58,  1.01it/s]\u001b[A\n",
            " 54% 68/126 [01:06<00:57,  1.02it/s]\u001b[A\n",
            " 55% 69/126 [01:07<00:57,  1.01s/it]\u001b[A\n",
            " 56% 70/126 [01:08<00:56,  1.00s/it]\u001b[A\n",
            " 56% 71/126 [01:09<00:54,  1.00it/s]\u001b[A\n",
            " 57% 72/126 [01:10<00:53,  1.01it/s]\u001b[A\n",
            " 58% 73/126 [01:11<00:52,  1.01it/s]\u001b[A\n",
            " 59% 74/126 [01:12<00:51,  1.01it/s]\u001b[A\n",
            " 60% 75/126 [01:13<00:50,  1.02it/s]\u001b[A\n",
            " 60% 76/126 [01:14<00:49,  1.02it/s]\u001b[A\n",
            " 61% 77/126 [01:15<00:49,  1.01s/it]\u001b[A\n",
            " 62% 78/126 [01:16<00:48,  1.00s/it]\u001b[A\n",
            " 63% 79/126 [01:17<00:46,  1.00it/s]\u001b[A\n",
            " 63% 80/126 [01:18<00:45,  1.01it/s]\u001b[A\n",
            " 64% 81/126 [01:19<00:44,  1.01it/s]\u001b[A\n",
            " 65% 82/126 [01:20<00:43,  1.02it/s]\u001b[A\n",
            " 66% 83/126 [01:21<00:42,  1.02it/s]\u001b[A\n",
            " 67% 84/126 [01:22<00:41,  1.02it/s]\u001b[A\n",
            " 67% 85/126 [01:23<00:40,  1.02it/s]\u001b[A\n",
            " 68% 86/126 [01:24<00:39,  1.02it/s]\u001b[A\n",
            " 69% 87/126 [01:25<00:38,  1.02it/s]\u001b[A\n",
            " 70% 88/126 [01:26<00:36,  1.05it/s]\u001b[A\n",
            " 71% 89/126 [01:26<00:35,  1.04it/s]\u001b[A\n",
            " 71% 90/126 [01:27<00:34,  1.03it/s]\u001b[A\n",
            " 72% 91/126 [01:28<00:33,  1.03it/s]\u001b[A\n",
            " 73% 92/126 [01:29<00:33,  1.03it/s]\u001b[A\n",
            " 74% 93/126 [01:30<00:32,  1.03it/s]\u001b[A\n",
            " 75% 94/126 [01:31<00:30,  1.05it/s]\u001b[A\n",
            " 75% 95/126 [01:32<00:29,  1.04it/s]\u001b[A\n",
            " 76% 96/126 [01:33<00:29,  1.03it/s]\u001b[A\n",
            " 77% 97/126 [01:34<00:28,  1.03it/s]\u001b[A\n",
            " 78% 98/126 [01:35<00:27,  1.02it/s]\u001b[A\n",
            " 79% 99/126 [01:36<00:26,  1.02it/s]\u001b[A\n",
            " 79% 100/126 [01:37<00:25,  1.02it/s]\u001b[A\n",
            " 80% 101/126 [01:38<00:24,  1.02it/s]\u001b[A\n",
            " 81% 102/126 [01:39<00:23,  1.02it/s]\u001b[A\n",
            " 82% 103/126 [01:40<00:22,  1.02it/s]\u001b[A\n",
            " 83% 104/126 [01:41<00:21,  1.02it/s]\u001b[A\n",
            " 83% 105/126 [01:42<00:20,  1.01it/s]\u001b[A\n",
            " 84% 106/126 [01:43<00:19,  1.01it/s]\u001b[A\n",
            " 85% 107/126 [01:44<00:19,  1.01s/it]\u001b[A\n",
            " 86% 108/126 [01:45<00:18,  1.00s/it]\u001b[A\n",
            " 87% 109/126 [01:46<00:16,  1.00it/s]\u001b[A\n",
            " 87% 110/126 [01:47<00:15,  1.01it/s]\u001b[A\n",
            " 88% 111/126 [01:48<00:14,  1.01it/s]\u001b[A\n",
            " 89% 112/126 [01:49<00:13,  1.02it/s]\u001b[A\n",
            " 90% 113/126 [01:50<00:12,  1.01it/s]\u001b[A\n",
            " 90% 114/126 [01:51<00:11,  1.01it/s]\u001b[A\n",
            " 91% 115/126 [01:52<00:10,  1.01it/s]\u001b[A\n",
            " 92% 116/126 [01:53<00:09,  1.02it/s]\u001b[A\n",
            " 93% 117/126 [01:54<00:08,  1.02it/s]\u001b[A\n",
            " 94% 118/126 [01:55<00:07,  1.02it/s]\u001b[A\n",
            " 94% 119/126 [01:56<00:06,  1.02it/s]\u001b[A\n",
            " 95% 120/126 [01:57<00:05,  1.02it/s]\u001b[A\n",
            " 96% 121/126 [01:58<00:04,  1.05it/s]\u001b[A\n",
            " 97% 122/126 [01:59<00:03,  1.04it/s]\u001b[A\n",
            " 98% 123/126 [02:00<00:02,  1.03it/s]\u001b[A\n",
            " 98% 124/126 [02:01<00:01,  1.03it/s]\u001b[A\n",
            " 99% 125/126 [02:02<00:00,  1.03it/s]\u001b[A\n",
            "                                           \n",
            "\u001b[A{'eval_turjuman_finetune_val_loss': 0.6256915330886841, 'eval_turjuman_finetune_val_runtime': 124.2289, 'eval_turjuman_finetune_val_samples_per_second': 1.014, 'eval_turjuman_finetune_val_steps_per_second': 1.014, 'epoch': 1.52}\n",
            " 51% 1100/2175 [4:02:51<3:29:37, 11.70s/it]\n",
            "100% 126/126 [02:03<00:00,  1.03it/s]\u001b[A\n",
            "                                     \u001b[A[INFO|trainer.py:3984] 2025-04-29 07:21:50,258 >> Saving model checkpoint to /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1100\n",
            "[INFO|configuration_utils.py:693] 2025-04-29 07:21:50,495 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\n",
            "[INFO|configuration_utils.py:765] 2025-04-29 07:21:50,496 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 1536,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8960,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 21,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 2,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 32768,\n",
            "  \"tie_word_embeddings\": true,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.51.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 151936\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 07:21:51,584 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 07:21:51,588 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/checkpoint-1100/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-29 07:21:58,279 >> tokenizer config file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-29 07:21:58,309 >> Special tokens file saved in /content/drive/MyDrive/finetunning/llamafactory-finetune-data/checkpoint/special_tokens_map.json\n",
            "{'loss': 0.4055, 'grad_norm': 1.4638984203338623, 'learning_rate': 5.700016111086651e-05, 'epoch': 1.53}\n",
            " 51% 1117/2175 [4:06:20<3:27:37, 11.77s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYMZ1yaUJUKO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "049a0b7c43fc440ab839c807e4ca1640": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "068a06a71149482f9f472cbb273cabad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3a1927bec14451798d8188b39718632",
            "placeholder": "​",
            "style": "IPY_MODEL_65c07644750d4e94aa3faee6442cb3f4",
            "value": "  5%"
          }
        },
        "072510185bcb4fc4853930a1573cfba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_663754bbbfcf4e1983acf014b5828434",
            "max": 242,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0ea96efb33554be9baf3bc0aaff82e5d",
            "value": 242
          }
        },
        "0b7138003452480a93ab3610941c3056": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ea96efb33554be9baf3bc0aaff82e5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1319d3163a944c6f931a0c1012edaf20": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ed2f1a5fc11473fac4bee3c3399232c",
            "placeholder": "​",
            "style": "IPY_MODEL_331543e3cc594f1a842dc16089f2d802",
            "value": " 3.09G/3.09G [00:29&lt;00:00, 105MB/s]"
          }
        },
        "186d0873d0144961a905eaa9568c77cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ca5d9b540344d4caf22ec494104a55c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331543e3cc594f1a842dc16089f2d802": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d61b0cc7774621a23b0268e9bd5ac2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a0b8636b7374d259cb23d943fbb40d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_049a0b7c43fc440ab839c807e4ca1640",
            "placeholder": "​",
            "style": "IPY_MODEL_78ef546ad13d4948b2705f67aa72fc83",
            "value": " 141/3000 [07:36&lt;2:38:18,  3.32s/it]"
          }
        },
        "51ffd949e7ad46148780456192906f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5cbe6af3f46f4055a942805f1f42a8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2f7c9137e95401aae2d9b460ec0ef90",
              "IPY_MODEL_072510185bcb4fc4853930a1573cfba1",
              "IPY_MODEL_cddb5bda3898441485c814e43c8dee29"
            ],
            "layout": "IPY_MODEL_2ca5d9b540344d4caf22ec494104a55c"
          }
        },
        "65c07644750d4e94aa3faee6442cb3f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "663754bbbfcf4e1983acf014b5828434": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ed2f1a5fc11473fac4bee3c3399232c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78ef546ad13d4948b2705f67aa72fc83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "795db8d889444f41bc9437dd527bd7d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c6341cec8374444b7d1518f826d23e1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c7c1585c5d0402d85716fef9079e0b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c634303bc314755837bfd524cbb01f4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c67280c5b254fd2ab07a951403df76f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96d706471cbc4f768df86f791aca3eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb5bf31e024a40d1a512d40415dc272e",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_186d0873d0144961a905eaa9568c77cc",
            "value": 141
          }
        },
        "984f2eb730414999980ad2c194a48905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c634303bc314755837bfd524cbb01f4",
            "placeholder": "​",
            "style": "IPY_MODEL_8c67280c5b254fd2ab07a951403df76f",
            "value": "model.safetensors: 100%"
          }
        },
        "aeb7b16a93044fc388a57d2b70b8d0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_068a06a71149482f9f472cbb273cabad",
              "IPY_MODEL_96d706471cbc4f768df86f791aca3eff",
              "IPY_MODEL_4a0b8636b7374d259cb23d943fbb40d4"
            ],
            "layout": "IPY_MODEL_fa4174ff50564676be17549f5ebf34c2"
          }
        },
        "b690dc04d98141208005679849a44bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_984f2eb730414999980ad2c194a48905",
              "IPY_MODEL_cd52f0ed0e7f425b8f496271b0291344",
              "IPY_MODEL_1319d3163a944c6f931a0c1012edaf20"
            ],
            "layout": "IPY_MODEL_7c7c1585c5d0402d85716fef9079e0b9"
          }
        },
        "b9c861ea25a84dc496f1a7da781ad77a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3a1927bec14451798d8188b39718632": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd52f0ed0e7f425b8f496271b0291344": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_795db8d889444f41bc9437dd527bd7d2",
            "max": 3087467144,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37d61b0cc7774621a23b0268e9bd5ac2",
            "value": 3087467144
          }
        },
        "cddb5bda3898441485c814e43c8dee29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7c6341cec8374444b7d1518f826d23e1",
            "placeholder": "​",
            "style": "IPY_MODEL_51ffd949e7ad46148780456192906f65",
            "value": " 242/242 [00:00&lt;00:00, 23.5kB/s]"
          }
        },
        "d2f7c9137e95401aae2d9b460ec0ef90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9c861ea25a84dc496f1a7da781ad77a",
            "placeholder": "​",
            "style": "IPY_MODEL_0b7138003452480a93ab3610941c3056",
            "value": "generation_config.json: 100%"
          }
        },
        "fa4174ff50564676be17549f5ebf34c2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb5bf31e024a40d1a512d40415dc272e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae91da5d7dbd44999c03ac40ff7705dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_e51acee8687c4240a86fdbac34f37f2d"
          }
        },
        "a3362e09ad10400b97986215bf8813a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_150973d1666c463493b86329c04ee62c",
            "placeholder": "​",
            "style": "IPY_MODEL_d4bee874eb72456fabbed6b5bb0ec43f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "bfb5392f2eac4b60a76d444b1e4e4270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_94f1f9487e024c2fb6367d59205df37d",
            "placeholder": "​",
            "style": "IPY_MODEL_7c6cba68c58a4e7e8ae6d358160d5bb8",
            "value": ""
          }
        },
        "3110b463a06941648205f7cce85e2e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_70fbdb5c9e904b07ad1dd83cc4e4aec0",
            "style": "IPY_MODEL_6ef95c78d0f24d2098536bc9ed5f206d",
            "value": true
          }
        },
        "d444e852b1c744e0902367b67d0867a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_f7902bbcd6944bb48a90e1b8916fb4ce",
            "style": "IPY_MODEL_425e62c367c04782abfb31a7be852779",
            "tooltip": ""
          }
        },
        "3dd65a3f37294b329957a59f020aceee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_addf90a3bb734298b23f9f377c4cd788",
            "placeholder": "​",
            "style": "IPY_MODEL_48ca2117758c4029a7e74bf683bebad4",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "e51acee8687c4240a86fdbac34f37f2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "150973d1666c463493b86329c04ee62c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4bee874eb72456fabbed6b5bb0ec43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94f1f9487e024c2fb6367d59205df37d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c6cba68c58a4e7e8ae6d358160d5bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70fbdb5c9e904b07ad1dd83cc4e4aec0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef95c78d0f24d2098536bc9ed5f206d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7902bbcd6944bb48a90e1b8916fb4ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "425e62c367c04782abfb31a7be852779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "addf90a3bb734298b23f9f377c4cd788": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48ca2117758c4029a7e74bf683bebad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b76c18a57ab4258ba9c7c749ca685a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0391850802474b91a11d920ce7c6f686",
            "placeholder": "​",
            "style": "IPY_MODEL_9b1336fd9c084439977383df0bfa6ba1",
            "value": "Connecting..."
          }
        },
        "0391850802474b91a11d920ce7c6f686": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b1336fd9c084439977383df0bfa6ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}